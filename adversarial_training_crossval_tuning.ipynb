{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39e24f3-a4f9-4650-9306-8d199f3480b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b0cca7-3f6a-4052-9339-b0cd7bf2f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('cleaned_profiles.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569d1a5b-0d9f-4256-a817-94a3aaaf9d12",
   "metadata": {},
   "source": [
    "XGBOOST - BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c3b289-50cd-471a-abee-2611382b3e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class XGBoostAdversarialTuning:\n",
    "    def __init__(self, embedding_file, dataset, model_name='roberta'):\n",
    "        # Store model name\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Prepare data\n",
    "        self.emb = pd.read_csv(embedding_file)\n",
    "        self.y = dataset[\"Label\"]\n",
    "        \n",
    "        # Prepare profile type masks\n",
    "        self.genuine_mask = self.y == 0\n",
    "        self.fake_manual_mask = self.y == 1\n",
    "        self.gptv1_mask = self.y.isin([10, 11])\n",
    "        self.gptv2_mask = self.y == 12\n",
    "        \n",
    "        # Initialize scaler\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Create models directory if it doesn't exist\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "\n",
    "    def _get_data_splits(self, random_state=42):\n",
    "        \"\"\"Prepare all data splits with 70-30 ratio\"\"\"\n",
    "        # Split genuine profiles (70-30)\n",
    "        X_genuine = self.emb[self.genuine_mask]\n",
    "        X_train_genuine, X_test_genuine = train_test_split(\n",
    "            X_genuine, test_size=0.3, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Split manual fake profiles (70-30)\n",
    "        X_fake = self.emb[self.fake_manual_mask]\n",
    "        X_train_fake, X_test_fake = train_test_split(\n",
    "            X_fake, test_size=0.3, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Split GPTv1 profiles (70-30)\n",
    "        X_gptv1 = self.emb[self.gptv1_mask]\n",
    "        X_train_gptv1, X_test_gptv1 = train_test_split(\n",
    "            X_gptv1, test_size=0.3, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Load GPTv2 profiles using the model-specific file from the embeddings_output directory\n",
    "        generated_file = f'embeddings_output/{self.model_name}_PCA_150_components_generated.csv'\n",
    "        gptv2_emb = pd.read_csv(generated_file)\n",
    "        X_train_gptv2, X_test_gptv2 = train_test_split(\n",
    "            gptv2_emb, test_size=0.3, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'train': {\n",
    "                'genuine': X_train_genuine,\n",
    "                'fake_manual': X_train_fake,\n",
    "                'gptv1': X_train_gptv1,\n",
    "                'gptv2': X_train_gptv2\n",
    "            },\n",
    "            'test': {\n",
    "                'genuine': X_test_genuine,\n",
    "                'fake_manual': X_test_fake,\n",
    "                'gptv1': X_test_gptv1,\n",
    "                'gptv2': X_test_gptv2\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _get_training_data(self, train_data, scenario):\n",
    "        \"\"\"Get training data for each scenario\"\"\"\n",
    "        if scenario == 'baseline':\n",
    "            X_train = pd.concat([\n",
    "                train_data['genuine'],\n",
    "                train_data['fake_manual']\n",
    "            ])\n",
    "            y_train = pd.Series([0]*len(train_data['genuine']) + [1]*len(train_data['fake_manual']))\n",
    "            \n",
    "        elif scenario == 'gptv1_assisted':\n",
    "            X_train = pd.concat([\n",
    "                train_data['genuine'],\n",
    "                train_data['fake_manual'],\n",
    "                train_data['gptv1']\n",
    "            ])\n",
    "            y_train = pd.Series([0]*len(train_data['genuine']) + \n",
    "                               [1]*(len(train_data['fake_manual']) + len(train_data['gptv1'])))\n",
    "            \n",
    "        elif scenario == 'gptv2_assisted':\n",
    "            X_train = pd.concat([\n",
    "                train_data['genuine'],\n",
    "                train_data['fake_manual'],\n",
    "                train_data['gptv2']\n",
    "            ])\n",
    "            y_train = pd.Series([0]*len(train_data['genuine']) + \n",
    "                               [1]*(len(train_data['fake_manual']) + len(train_data['gptv2'])))\n",
    "            \n",
    "        elif scenario == 'gptv1v2_assisted':\n",
    "            X_train = pd.concat([\n",
    "                train_data['genuine'],\n",
    "                train_data['fake_manual'],\n",
    "                train_data['gptv1'],\n",
    "                train_data['gptv2']\n",
    "            ])\n",
    "            y_train = pd.Series([0]*len(train_data['genuine']) + \n",
    "                               [1]*(len(train_data['fake_manual']) + \n",
    "                                   len(train_data['gptv1']) + len(train_data['gptv2'])))\n",
    "        \n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        return X_train_scaled, y_train\n",
    "\n",
    "    def _get_test_data(self, test_data, scenario):\n",
    "        \"\"\"Get test data for each scenario\"\"\"\n",
    "        if scenario == 'baseline':\n",
    "            X_test = pd.concat([\n",
    "                test_data['genuine'],\n",
    "                test_data['fake_manual']\n",
    "            ]).reset_index(drop=True)\n",
    "            y_test = pd.Series([0]*len(test_data['genuine']) + [1]*len(test_data['fake_manual']))\n",
    "            \n",
    "        elif scenario == 'gptv1':\n",
    "            X_test = pd.concat([\n",
    "                test_data['genuine'],\n",
    "                test_data['fake_manual'],\n",
    "                test_data['gptv1']\n",
    "            ]).reset_index(drop=True)\n",
    "            y_test = pd.Series([0]*len(test_data['genuine']) + \n",
    "                              [1]*(len(test_data['fake_manual']) + len(test_data['gptv1'])))\n",
    "            \n",
    "        elif scenario == 'gptv2':\n",
    "            X_test = pd.concat([\n",
    "                test_data['genuine'],\n",
    "                test_data['fake_manual'],\n",
    "                test_data['gptv2']\n",
    "            ]).reset_index(drop=True)\n",
    "            y_test = pd.Series([0]*len(test_data['genuine']) + \n",
    "                              [1]*(len(test_data['fake_manual']) + len(test_data['gptv2'])))\n",
    "            \n",
    "        else:  # gptv1v2\n",
    "            X_test = pd.concat([\n",
    "                test_data['genuine'],\n",
    "                test_data['fake_manual'],\n",
    "                test_data['gptv1'],\n",
    "                test_data['gptv2']\n",
    "            ]).reset_index(drop=True)\n",
    "            y_test = pd.Series([0]*len(test_data['genuine']) + \n",
    "                              [1]*(len(test_data['fake_manual']) + \n",
    "                                  len(test_data['gptv1']) + len(test_data['gptv2'])))\n",
    "        \n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        return X_test_scaled, y_test\n",
    "\n",
    "    def _calculate_metrics(self, y_true, y_pred):\n",
    "        \"\"\"Calculate F1, FAR, and FRR\"\"\"\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        far = fp / (fp + tn)\n",
    "        frr = fn / (fn + tp)\n",
    "        return {'f1_score': f1, 'far': far, 'frr': frr}\n",
    "\n",
    "    def bayesian_optimization_xgboost(self, X_train, y_train):\n",
    "        \"\"\"Two-phase Bayesian optimization for XGBoost with GPU acceleration\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        print(f\"\\nStarting Two-Phase Bayesian Optimization at {start_time.strftime('%H:%M:%S')}\")\n",
    "    \n",
    "        # Split training data for quick validation in Phase 1\n",
    "        X_train_quick, X_val_quick, y_train_quick, y_val_quick = train_test_split(\n",
    "            X_train, y_train, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Create a separate validation set for final model training (to avoid test data leakage)\n",
    "        X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(\n",
    "            X_train, y_train, test_size=0.15, random_state=42\n",
    "        )\n",
    "        \n",
    "        best_score_so_far = 0\n",
    "        trial_times = []\n",
    "    \n",
    "        def objective(trial):\n",
    "            nonlocal best_score_so_far\n",
    "            trial_start = datetime.now()\n",
    "            \n",
    "            # Determine which phase we're in\n",
    "            is_phase_2 = trial.number >= 30\n",
    "            phase_name = \"Phase 2 (CV)\" if is_phase_2 else \"Phase 1 (Quick)\"\n",
    "            trial_in_phase = trial.number - 30 + 1 if is_phase_2 else trial.number + 1\n",
    "            max_trials_in_phase = 20 if is_phase_2 else 30\n",
    "            \n",
    "            print(f\"\\n{phase_name} - Trial {trial_in_phase}/{max_trials_in_phase} started at {trial_start.strftime('%H:%M:%S')}\")\n",
    "    \n",
    "            # Define parameter space\n",
    "            params = {\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "                'learning_rate': trial.suggest_categorical('learning_rate', [0.01, 0.05, 0.1, 0.2, 0.3]),\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 550, 50),\n",
    "                'subsample': trial.suggest_categorical('subsample', [0.6, 0.7, 0.8, 0.9, 1.0]),\n",
    "                'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.6, 0.7, 0.8, 0.9, 1.0]),\n",
    "                'tree_method': 'gpu_hist',  # Changed from 'hist' to 'gpu_hist' for GPU acceleration\n",
    "                'gpu_id': 0,  # Specify GPU device ID (usually 0 for the first GPU)\n",
    "                'predictor': 'gpu_predictor',  # Use GPU for prediction\n",
    "                'random_state': 42\n",
    "            }\n",
    "    \n",
    "            try:\n",
    "                clf = XGBClassifier(**params)\n",
    "            \n",
    "                # Phase 1: Quick validation\n",
    "                if not is_phase_2:\n",
    "                    clf.fit(X_train_quick, y_train_quick,\n",
    "                           eval_set=[(X_val_quick, y_val_quick)],\n",
    "                           verbose=False)\n",
    "                    score = f1_score(y_val_quick, clf.predict(X_val_quick), average='weighted')\n",
    "                # Phase 2: 5-fold CV\n",
    "                else:\n",
    "                    score = cross_val_score(clf, X_train, y_train, cv=5, scoring='f1').mean()\n",
    "                \n",
    "                trial_end = datetime.now()\n",
    "                trial_duration = trial_end - trial_start\n",
    "                trial_times.append(trial_duration.total_seconds())\n",
    "                avg_trial_time = np.mean(trial_times[-5:]) if len(trial_times) > 5 else np.mean(trial_times)\n",
    "                \n",
    "                best_score_so_far = max(best_score_so_far, score)\n",
    "                print(f\"Trial {trial_in_phase} results:\")\n",
    "                print(f\"Score: {score:.4f}\")\n",
    "                print(f\"Best score so far: {best_score_so_far:.4f}\")\n",
    "                print(f\"Trial duration: {trial_duration}\")\n",
    "                print(f\"Average trial duration: {avg_trial_time:.2f} seconds\")\n",
    "                \n",
    "                remaining_in_phase = max_trials_in_phase - trial_in_phase\n",
    "                remaining_time = avg_trial_time * remaining_in_phase\n",
    "                print(f\"Estimated time remaining in {phase_name}: {remaining_time:.2f} seconds\")\n",
    "            \n",
    "                return score\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in trial: {str(e)}\")\n",
    "                return 0\n",
    "    \n",
    "        # Create and run study\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        print(f\"\\nPhase 1: Quick Evaluation with Single Validation Split (30 trials)\")\n",
    "        print(f\"Phase 2: Fine-tuning with 5-fold CV (20 trials)\")\n",
    "        study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "    \n",
    "        # Final evaluation - using a separate validation set instead of test data for early stopping\n",
    "        final_model = XGBClassifier(**study.best_params)\n",
    "        final_model.fit(X_train_final, y_train_final, \n",
    "                       eval_set=[(X_val_final, y_val_final)], \n",
    "                       verbose=False)\n",
    "    \n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - start_time\n",
    "        print(f\"\\nBayesian Optimization completed at {end_time.strftime('%H:%M:%S')}\")\n",
    "        print(f\"Total duration: {duration}\")\n",
    "        print(f\"Best parameters: {study.best_params}\")\n",
    "    \n",
    "        return study.best_params, final_model\n",
    "    \n",
    "    def train_and_evaluate(self, embedding_file, load_existing=False, model_name=None):\n",
    "        \"\"\"Main method to train and evaluate across scenarios\"\"\"\n",
    "        # Use model name in the saved file path if provided\n",
    "        model_prefix = f\"{self.model_name}_\" if model_name is None else f\"{model_name}_\"\n",
    "        model_path = f'models/{model_prefix}xgb_bo_models.pkl'\n",
    "        \n",
    "        if load_existing and os.path.exists(model_path):\n",
    "            print(\"Loading existing models and results...\")\n",
    "            with open(model_path, 'rb') as f:\n",
    "                saved_data = pickle.load(f)\n",
    "                \n",
    "            # Return the loaded data\n",
    "            return saved_data['results']\n",
    "        \n",
    "        splits = self._get_data_splits()\n",
    "        training_scenarios = ['baseline', 'gptv1_assisted', 'gptv2_assisted', 'gptv1v2_assisted']\n",
    "        test_scenarios = ['baseline', 'gptv1', 'gptv2', 'gptv1v2']\n",
    "        \n",
    "        # Store models, parameters, scalers, and results\n",
    "        all_models = {}\n",
    "        all_params = {}\n",
    "        all_scalers = {}\n",
    "        all_results = {}\n",
    "        \n",
    "        for train_scenario in training_scenarios:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Training scenario: {train_scenario}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            X_train, y_train = self._get_training_data(splits['train'], train_scenario)\n",
    "            \n",
    "            # Store the scaler for this scenario\n",
    "            all_scalers[train_scenario] = self.scaler\n",
    "            \n",
    "            # Optimize XGBoost for this training scenario\n",
    "            best_params, model = self.bayesian_optimization_xgboost(X_train, y_train)\n",
    "            \n",
    "            # Store the model and parameters\n",
    "            all_models[train_scenario] = model\n",
    "            all_params[train_scenario] = best_params\n",
    "            \n",
    "            scenario_results = {}\n",
    "            for test_scenario in test_scenarios:\n",
    "                X_test, y_test = self._get_test_data(splits['test'], test_scenario)\n",
    "                \n",
    "                # Get predictions and metrics\n",
    "                y_pred = model.predict(X_test)\n",
    "                scenario_results[test_scenario] = self._calculate_metrics(y_test, y_pred)\n",
    "                \n",
    "            all_results[train_scenario] = scenario_results\n",
    "            \n",
    "        # Save all components\n",
    "        save_data = {\n",
    "            'models': all_models,\n",
    "            'params': all_params,\n",
    "            'scalers': all_scalers,\n",
    "            'results': all_results\n",
    "        }\n",
    "        \n",
    "        # Use model name in the saved file paths\n",
    "        model_prefix = f\"{self.model_name}_\"\n",
    "        model_path = f'models/{model_prefix}xgb_bo_models.pkl'\n",
    "        results_path = f'models/{model_prefix}xgb_bo.pkl'\n",
    "        \n",
    "        # Save all to a single file\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(save_data, f)\n",
    "        \n",
    "        # Also save only the results separately (for backward compatibility)\n",
    "        with open(results_path, 'wb') as f:\n",
    "            pickle.dump(all_results, f)\n",
    "            \n",
    "        print(f\"\\nModels, parameters, scalers, and results saved to '{model_path}'\")\n",
    "        print(f\"Results also saved to '{results_path}' for backward compatibility\")\n",
    "        \n",
    "        return all_results\n",
    "\n",
    "    def print_results(self, results):\n",
    "        \"\"\"Print results in a formatted manner\"\"\"\n",
    "        print(\"\\nDetailed Results:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Metrics to print\n",
    "        metrics = ['f1_score', 'far', 'frr']\n",
    "        \n",
    "        # Get unique training scenarios from results dict\n",
    "        train_scenarios = list(results.keys())\n",
    "        test_scenarios = ['baseline', 'gptv1', 'gptv2', 'gptv1v2']\n",
    "        \n",
    "        for metric in metrics:\n",
    "            print(f\"\\n{metric.upper()} Results:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"{'Training Scenario':<20} {'Baseline':<15} {'GPTv1':<15} {'GPTv2':<15} {'GPTv1v2':<15}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for train_scenario in train_scenarios:\n",
    "                metrics_row = [results[train_scenario][test][metric] \n",
    "                               for test in test_scenarios]\n",
    "                print(f\"{train_scenario:<20} \" + \" \".join(f\"{m:.4f}{' '*8}\" for m in metrics_row))\n",
    "                \n",
    "    def load_models(self, model_name=None):\n",
    "        \"\"\"Load saved models and data\"\"\"\n",
    "        # Use model name in the file path\n",
    "        model_to_load = model_name if model_name else self.model_name\n",
    "        model_path = f'models/{model_to_load}_xgb_bo_models.pkl'\n",
    "        \n",
    "        if os.path.exists(model_path):\n",
    "            with open(model_path, 'rb') as f:\n",
    "                saved_data = pickle.load(f)\n",
    "            print(f\"Models for {model_to_load} loaded successfully.\")\n",
    "            return saved_data\n",
    "        else:\n",
    "            print(f\"No saved models found for {model_to_load}.\")\n",
    "            return None\n",
    "\n",
    "def main():\n",
    "    # Specify which model to use\n",
    "    model_name = 'roberta'  # Change this to 'deberta', 'modernbert', or 'flair' as needed\n",
    "    \n",
    "    # Load embedding file based on the model\n",
    "    embedding_file = f'embeddings_output/{model_name}_PCA_150_components.csv'\n",
    "    \n",
    "    # Assuming dataset is a DataFrame with a 'Label' column\n",
    "    # You need to properly define this based on your data structure\n",
    "    #dataset = pd.read_csv('your_dataset.csv')  # Replace with actual dataset path\n",
    "    \n",
    "    # Initialize the tuner with the embedding file, dataset, and model name\n",
    "    tuner = XGBoostAdversarialTuning(embedding_file, dataset, model_name)\n",
    "    \n",
    "    # Choose whether to load existing models or train new ones\n",
    "    load_existing = False  # Set to True to load previously saved models\n",
    "    \n",
    "    model_path = f'models/{model_name}_xgb_bo_models.pkl'\n",
    "    \n",
    "    if load_existing and os.path.exists(model_path):\n",
    "        # Load existing models and results\n",
    "        saved_data = tuner.load_models(model_name)\n",
    "        # Print results from the saved data\n",
    "        tuner.print_results(saved_data['results'])\n",
    "    else:\n",
    "        # Train and evaluate new models\n",
    "        results = tuner.train_and_evaluate(embedding_file, load_existing=False, model_name=model_name)\n",
    "        # Print the results\n",
    "        tuner.print_results(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31482f6a-0f59-475a-a955-d1dd4015647c",
   "metadata": {},
   "source": [
    "XGBOOST - GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dd2661-f030-4b21-a625-b6c95534b144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
