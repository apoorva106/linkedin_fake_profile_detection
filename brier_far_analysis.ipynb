{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0c5317d-c341-4fb4-be80-ffc751926a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 14:30:46.335969: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-30 14:30:46.387652: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-30 14:30:46.387686: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-30 14:30:46.389269: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-30 14:30:46.400069: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "638be04d-3f30-48dd-b8f8-b7725082b704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intro</th>\n",
       "      <th>Full Name</th>\n",
       "      <th>Workplace</th>\n",
       "      <th>Location</th>\n",
       "      <th>Connections</th>\n",
       "      <th>Photo</th>\n",
       "      <th>Followers</th>\n",
       "      <th>About</th>\n",
       "      <th>Experiences</th>\n",
       "      <th>Number of Experiences</th>\n",
       "      <th>...</th>\n",
       "      <th>Number of Scores</th>\n",
       "      <th>Languages</th>\n",
       "      <th>Number of Languages</th>\n",
       "      <th>Organizations</th>\n",
       "      <th>Number of Organizations</th>\n",
       "      <th>Interests</th>\n",
       "      <th>Number of Interests</th>\n",
       "      <th>Activities</th>\n",
       "      <th>Number of Activities</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'Full Name': 'chenxia (polly) Pei', 'Workplac...</td>\n",
       "      <td>chenxia (polly) Pei</td>\n",
       "      <td>Jiangsu Junyao mainly offer services to cement...</td>\n",
       "      <td>Wuxi, Jiangsu, China</td>\n",
       "      <td>500</td>\n",
       "      <td>No</td>\n",
       "      <td>717</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'0': {'Role': 'Sales', 'Workplace': 'Jiangsu ...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{'0': {'Interest': 'Trina Solar', 'ID': '69648...</td>\n",
       "      <td>4</td>\n",
       "      <td>{'chenxia-pei-80177594': {'Full Name': 'chenxi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'Full Name': 'NEHA CHANDOK', 'Workplace': 'So...</td>\n",
       "      <td>NEHA CHANDOK</td>\n",
       "      <td>Software Analyst</td>\n",
       "      <td>Noida, Uttar Pradesh, India</td>\n",
       "      <td>500</td>\n",
       "      <td>No</td>\n",
       "      <td>1340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'0': {'Role': 'Software Analyst', 'Workplace'...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Language 0': 'Bangla', 'Language 1': 'Englis...</td>\n",
       "      <td>3</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{'0': {'Interest': 'Mohamed El-Erian', 'ID': '...</td>\n",
       "      <td>8</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'Full Name': 'Mounika Mungamuri', 'Workplace'...</td>\n",
       "      <td>Mounika Mungamuri</td>\n",
       "      <td>Senior Consultant at Infosys</td>\n",
       "      <td>Hyderabad, Telangana, India</td>\n",
       "      <td>7</td>\n",
       "      <td>Yes</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'0': {'Role': 'Senior Consultant', 'Workplace...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'Full Name': 'Katarina Djuric', 'Workplace': ...</td>\n",
       "      <td>Katarina Djuric</td>\n",
       "      <td>--</td>\n",
       "      <td>Belgrade, Serbia</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'0': {'Role': 'Instructor', 'Workplace': 'GE'...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{'0': {'Interest': 'GE', 'ID': '1015', 'Catego...</td>\n",
       "      <td>1</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'Full Name': 'Rachel Lally', 'Workplace': '--...</td>\n",
       "      <td>Rachel Lally</td>\n",
       "      <td>--</td>\n",
       "      <td>Dublin, County Dublin, Ireland</td>\n",
       "      <td>61</td>\n",
       "      <td>Yes</td>\n",
       "      <td>61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'0': {'Role': 'Bartender', 'Workplace': \"O'Su...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{'0': {'Interest': 'Richard Branson', 'ID': 'r...</td>\n",
       "      <td>3</td>\n",
       "      <td>{'garyltravis': {'Full Name': 'Gary Travis', '...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>{'Full Name': 'Emily Williams', 'Workplace': \"...</td>\n",
       "      <td>Emily Williams</td>\n",
       "      <td>Market Research Analyst at L'Oreal</td>\n",
       "      <td>New York City, New York</td>\n",
       "      <td>106</td>\n",
       "      <td>No</td>\n",
       "      <td>717</td>\n",
       "      <td>{}</td>\n",
       "      <td>Market Research Analyst Elizabeth Arden Jan 20...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>3</td>\n",
       "      <td>{}</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>{'Full Name': 'Sarah Johnson', 'Workplace': 'D...</td>\n",
       "      <td>Sarah Johnson</td>\n",
       "      <td>Director of Marketing Strategy at Acme Inc.</td>\n",
       "      <td>San Francisco, California</td>\n",
       "      <td>102</td>\n",
       "      <td>No</td>\n",
       "      <td>6</td>\n",
       "      <td>{}</td>\n",
       "      <td>Director of Marketing Acme Inc. January 2018-P...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>5</td>\n",
       "      <td>{}</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>{'Full Name': 'Sarah Johnson', 'Workplace': 'M...</td>\n",
       "      <td>Sarah Johnson</td>\n",
       "      <td>Manager at Acme Inc.</td>\n",
       "      <td>New York City, New York</td>\n",
       "      <td>435</td>\n",
       "      <td>No</td>\n",
       "      <td>10</td>\n",
       "      <td>A results-driven and experienced professional ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>6</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>{'Full Name': 'Emma Williams', 'Workplace': 'M...</td>\n",
       "      <td>Emma Williams</td>\n",
       "      <td>Manager and Director at ABC Inc.</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>280</td>\n",
       "      <td>No</td>\n",
       "      <td>34</td>\n",
       "      <td>With over 10 years of experience in operations...</td>\n",
       "      <td>Operations Manager ABC Inc. January 2015- Pres...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>English Spanish</td>\n",
       "      <td>2</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>4</td>\n",
       "      <td>{}</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>{'Full Name': 'Rachel Johnson', 'Workplace': '...</td>\n",
       "      <td>Rachel Johnson</td>\n",
       "      <td>Market Development Manager at ABC Inc.</td>\n",
       "      <td>Dhaka, Bangladesh</td>\n",
       "      <td>102</td>\n",
       "      <td>No</td>\n",
       "      <td>26</td>\n",
       "      <td>{}</td>\n",
       "      <td>Market Development Manager ABC Inc. January 20...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>2</td>\n",
       "      <td>{}</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3600 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Intro            Full Name  \\\n",
       "0     {'Full Name': 'chenxia (polly) Pei', 'Workplac...  chenxia (polly) Pei   \n",
       "1     {'Full Name': 'NEHA CHANDOK', 'Workplace': 'So...         NEHA CHANDOK   \n",
       "2     {'Full Name': 'Mounika Mungamuri', 'Workplace'...    Mounika Mungamuri   \n",
       "3     {'Full Name': 'Katarina Djuric', 'Workplace': ...      Katarina Djuric   \n",
       "4     {'Full Name': 'Rachel Lally', 'Workplace': '--...         Rachel Lally   \n",
       "...                                                 ...                  ...   \n",
       "3595  {'Full Name': 'Emily Williams', 'Workplace': \"...       Emily Williams   \n",
       "3596  {'Full Name': 'Sarah Johnson', 'Workplace': 'D...        Sarah Johnson   \n",
       "3597  {'Full Name': 'Sarah Johnson', 'Workplace': 'M...        Sarah Johnson   \n",
       "3598  {'Full Name': 'Emma Williams', 'Workplace': 'M...        Emma Williams   \n",
       "3599  {'Full Name': 'Rachel Johnson', 'Workplace': '...       Rachel Johnson   \n",
       "\n",
       "                                              Workplace  \\\n",
       "0     Jiangsu Junyao mainly offer services to cement...   \n",
       "1                                      Software Analyst   \n",
       "2                          Senior Consultant at Infosys   \n",
       "3                                                    --   \n",
       "4                                                    --   \n",
       "...                                                 ...   \n",
       "3595                 Market Research Analyst at L'Oreal   \n",
       "3596        Director of Marketing Strategy at Acme Inc.   \n",
       "3597                               Manager at Acme Inc.   \n",
       "3598                   Manager and Director at ABC Inc.   \n",
       "3599             Market Development Manager at ABC Inc.   \n",
       "\n",
       "                            Location  Connections Photo  Followers  \\\n",
       "0               Wuxi, Jiangsu, China          500    No        717   \n",
       "1        Noida, Uttar Pradesh, India          500    No       1340   \n",
       "2        Hyderabad, Telangana, India            7   Yes          7   \n",
       "3                   Belgrade, Serbia            0   Yes          0   \n",
       "4     Dublin, County Dublin, Ireland           61   Yes         61   \n",
       "...                              ...          ...   ...        ...   \n",
       "3595         New York City, New York          106    No        717   \n",
       "3596       San Francisco, California          102    No          6   \n",
       "3597         New York City, New York          435    No         10   \n",
       "3598             Seattle, Washington          280    No         34   \n",
       "3599               Dhaka, Bangladesh          102    No         26   \n",
       "\n",
       "                                                  About  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "3595                                                 {}   \n",
       "3596                                                 {}   \n",
       "3597  A results-driven and experienced professional ...   \n",
       "3598  With over 10 years of experience in operations...   \n",
       "3599                                                 {}   \n",
       "\n",
       "                                            Experiences  \\\n",
       "0     {'0': {'Role': 'Sales', 'Workplace': 'Jiangsu ...   \n",
       "1     {'0': {'Role': 'Software Analyst', 'Workplace'...   \n",
       "2     {'0': {'Role': 'Senior Consultant', 'Workplace...   \n",
       "3     {'0': {'Role': 'Instructor', 'Workplace': 'GE'...   \n",
       "4     {'0': {'Role': 'Bartender', 'Workplace': \"O'Su...   \n",
       "...                                                 ...   \n",
       "3595  Market Research Analyst Elizabeth Arden Jan 20...   \n",
       "3596  Director of Marketing Acme Inc. January 2018-P...   \n",
       "3597                                                 {}   \n",
       "3598  Operations Manager ABC Inc. January 2015- Pres...   \n",
       "3599  Market Development Manager ABC Inc. January 20...   \n",
       "\n",
       "      Number of Experiences  ... Number of Scores  \\\n",
       "0                         2  ...                0   \n",
       "1                         1  ...                0   \n",
       "2                         2  ...                0   \n",
       "3                         1  ...                0   \n",
       "4                         1  ...                1   \n",
       "...                     ...  ...              ...   \n",
       "3595                      3  ...                0   \n",
       "3596                      5  ...                0   \n",
       "3597                      0  ...                0   \n",
       "3598                      1  ...                0   \n",
       "3599                      5  ...                0   \n",
       "\n",
       "                                              Languages Number of Languages  \\\n",
       "0                                                    {}                   0   \n",
       "1     {'Language 0': 'Bangla', 'Language 1': 'Englis...                   3   \n",
       "2                                                    {}                   0   \n",
       "3                                                    {}                   0   \n",
       "4                                                    {}                   0   \n",
       "...                                                 ...                 ...   \n",
       "3595                                                 {}                   0   \n",
       "3596                                                 {}                   0   \n",
       "3597                                                 {}                   0   \n",
       "3598                                    English Spanish                   2   \n",
       "3599                                                 {}                   0   \n",
       "\n",
       "      Organizations Number of Organizations  \\\n",
       "0                {}                       0   \n",
       "1                {}                       0   \n",
       "2                {}                       0   \n",
       "3                {}                       0   \n",
       "4                {}                       0   \n",
       "...             ...                     ...   \n",
       "3595             {}                       0   \n",
       "3596             {}                       0   \n",
       "3597             {}                       0   \n",
       "3598             {}                       0   \n",
       "3599             {}                       0   \n",
       "\n",
       "                                              Interests Number of Interests  \\\n",
       "0     {'0': {'Interest': 'Trina Solar', 'ID': '69648...                   4   \n",
       "1     {'0': {'Interest': 'Mohamed El-Erian', 'ID': '...                   8   \n",
       "2                                                    {}                   0   \n",
       "3     {'0': {'Interest': 'GE', 'ID': '1015', 'Catego...                   1   \n",
       "4     {'0': {'Interest': 'Richard Branson', 'ID': 'r...                   3   \n",
       "...                                                 ...                 ...   \n",
       "3595                                                 {}                   3   \n",
       "3596                                                 {}                   5   \n",
       "3597                                                 {}                   6   \n",
       "3598                                                 {}                   4   \n",
       "3599                                                 {}                   2   \n",
       "\n",
       "                                             Activities Number of Activities  \\\n",
       "0     {'chenxia-pei-80177594': {'Full Name': 'chenxi...                    1   \n",
       "1                                                    {}                    0   \n",
       "2                                                    {}                    0   \n",
       "3                                                    {}                    0   \n",
       "4     {'garyltravis': {'Full Name': 'Gary Travis', '...                    6   \n",
       "...                                                 ...                  ...   \n",
       "3595                                                 {}                    4   \n",
       "3596                                                 {}                    3   \n",
       "3597                                                 {}                    0   \n",
       "3598                                                 {}                    4   \n",
       "3599                                                 {}                   10   \n",
       "\n",
       "      Label  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  \n",
       "...     ...  \n",
       "3595     11  \n",
       "3596     11  \n",
       "3597     11  \n",
       "3598     11  \n",
       "3599     11  \n",
       "\n",
       "[3600 rows x 39 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read from CSV instead of pickle\n",
    "dataset = pd.read_csv('cleaned_profiles.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4692d935-d6d0-4271-8da6-c74b89bd80ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================== ANALYZING ROBERTA ==============================\n",
      "Loaded embeddings from embeddings_output/roberta_PCA_150_components.csv\n",
      "\n",
      "Evaluating svc classifier with roberta embeddings...\n",
      "  Training scenario: baseline\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv1_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv2_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv1v2_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "\n",
      "Evaluating logistic_regression classifier with roberta embeddings...\n",
      "  Training scenario: baseline\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv1_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv2_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv1v2_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "\n",
      "Evaluating catboost classifier with roberta embeddings...\n",
      "  Training scenario: baseline\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv1_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv2_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv1v2_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "Exported results to brier_far_analysis/roberta_brier_far_results.csv\n",
      "Exported correlation results to brier_far_analysis/roberta_brier_far_correlations.csv\n",
      "Saved plot to brier_far_analysis/roberta_svc_brier_vs_far.png\n",
      "Saved plot to brier_far_analysis/roberta_logistic_regression_brier_vs_far.png\n",
      "Saved plot to brier_far_analysis/roberta_catboost_brier_vs_far.png\n",
      "Saved combined plot to brier_far_analysis/roberta_all_classifiers_brier_vs_far.png\n",
      "Saved Brier score heatmap to brier_far_analysis/roberta_brier_heatmap.png\n",
      "Saved FAR heatmap to brier_far_analysis/roberta_far_heatmap.png\n",
      "\n",
      "============================== ANALYZING MODERNBERT ==============================\n",
      "Loaded embeddings from embeddings_output/modernbert_PCA_150_components.csv\n",
      "\n",
      "Evaluating svc classifier with modernbert embeddings...\n",
      "  Training scenario: baseline\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv1_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv2_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv1v2_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "\n",
      "Evaluating logistic_regression classifier with modernbert embeddings...\n",
      "  Training scenario: baseline\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv1_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv2_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv1v2_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "\n",
      "Evaluating catboost classifier with modernbert embeddings...\n",
      "  Training scenario: baseline\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv1_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv2_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv1v2_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "Exported results to brier_far_analysis/modernbert_brier_far_results.csv\n",
      "Exported correlation results to brier_far_analysis/modernbert_brier_far_correlations.csv\n",
      "Saved plot to brier_far_analysis/modernbert_svc_brier_vs_far.png\n",
      "Saved plot to brier_far_analysis/modernbert_logistic_regression_brier_vs_far.png\n",
      "Saved plot to brier_far_analysis/modernbert_catboost_brier_vs_far.png\n",
      "Saved combined plot to brier_far_analysis/modernbert_all_classifiers_brier_vs_far.png\n",
      "Saved Brier score heatmap to brier_far_analysis/modernbert_brier_heatmap.png\n",
      "Saved FAR heatmap to brier_far_analysis/modernbert_far_heatmap.png\n",
      "\n",
      "============================== ANALYZING DEBERTA ==============================\n",
      "Loaded embeddings from embeddings_output/deberta_PCA_150_components.csv\n",
      "\n",
      "Evaluating svc classifier with deberta embeddings...\n",
      "  Training scenario: baseline\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv1_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv2_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv1v2_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "\n",
      "Evaluating logistic_regression classifier with deberta embeddings...\n",
      "  Training scenario: baseline\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv1_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv2_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv1v2_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "\n",
      "Evaluating catboost classifier with deberta embeddings...\n",
      "  Training scenario: baseline\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv1_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv2_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv1v2_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "Exported results to brier_far_analysis/deberta_brier_far_results.csv\n",
      "Exported correlation results to brier_far_analysis/deberta_brier_far_correlations.csv\n",
      "Saved plot to brier_far_analysis/deberta_svc_brier_vs_far.png\n",
      "Saved plot to brier_far_analysis/deberta_logistic_regression_brier_vs_far.png\n",
      "Saved plot to brier_far_analysis/deberta_catboost_brier_vs_far.png\n",
      "Saved combined plot to brier_far_analysis/deberta_all_classifiers_brier_vs_far.png\n",
      "Saved Brier score heatmap to brier_far_analysis/deberta_brier_heatmap.png\n",
      "Saved FAR heatmap to brier_far_analysis/deberta_far_heatmap.png\n",
      "\n",
      "============================== ANALYZING FLAIR ==============================\n",
      "Loaded embeddings from embeddings_output/flair_PCA_150_components.csv\n",
      "\n",
      "Evaluating svc classifier with flair embeddings...\n",
      "  Training scenario: baseline\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv1_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv2_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv1v2_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "\n",
      "Evaluating logistic_regression classifier with flair embeddings...\n",
      "  Training scenario: baseline\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv1_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv2_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv1v2_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "\n",
      "Evaluating catboost classifier with flair embeddings...\n",
      "  Training scenario: baseline\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv1_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv2_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "  Training scenario: gptv1v2_assisted\n",
      "    Testing against: baseline\n",
      "    Testing against: gptv1\n",
      "    Testing against: gptv2\n",
      "    Testing against: gptv1v2\n",
      "Exported results to brier_far_analysis/flair_brier_far_results.csv\n",
      "Exported correlation results to brier_far_analysis/flair_brier_far_correlations.csv\n",
      "Saved plot to brier_far_analysis/flair_svc_brier_vs_far.png\n",
      "Saved plot to brier_far_analysis/flair_logistic_regression_brier_vs_far.png\n",
      "Saved plot to brier_far_analysis/flair_catboost_brier_vs_far.png\n",
      "Saved combined plot to brier_far_analysis/flair_all_classifiers_brier_vs_far.png\n",
      "Saved Brier score heatmap to brier_far_analysis/flair_brier_heatmap.png\n",
      "Saved FAR heatmap to brier_far_analysis/flair_far_heatmap.png\n",
      "Exported combined results to brier_far_analysis/combined_brier_far_results.csv\n",
      "Exported combined correlation results to brier_far_analysis/combined_brier_far_correlations.csv\n",
      "Saved combined plot for all models to brier_far_analysis/all_models_classifiers_brier_vs_far.png\n",
      "\n",
      "Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, brier_score_loss\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import os\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "def get_classifier(classifier_name, random_state=42):\n",
    "    \"\"\"Return classifier instance based on name\"\"\"\n",
    "    if classifier_name == 'svc':\n",
    "        return SVC(random_state=random_state, probability=True)\n",
    "    elif classifier_name == 'logistic_regression':\n",
    "        return LogisticRegression(random_state=random_state, max_iter=1000)\n",
    "    elif classifier_name == 'catboost':\n",
    "        return CatBoostClassifier(random_state=random_state, verbose=False)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_prob=None):\n",
    "    \"\"\"Calculate F1, FAR, and FRR, plus Brier score if probabilities provided\"\"\"\n",
    "    metrics = {}\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    frr = fp / (fp + tn)\n",
    "    far = fn / (fn + tp)\n",
    "    \n",
    "    metrics = {'f1_score': f1, 'far': far, 'frr': frr}\n",
    "    \n",
    "    # Calculate Brier score if probabilities provided\n",
    "    if y_prob is not None:\n",
    "        brier = brier_score_loss(y_true, y_prob)\n",
    "        metrics['brier_score'] = brier\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "def create_results_dir():\n",
    "    \"\"\"Create results directory if it doesn't exist\"\"\"\n",
    "    results_dir = 'brier_far_analysis'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    return results_dir\n",
    "\n",
    "class BrierFARAnalysis:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.results_dir = create_results_dir()\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Define classifiers with fixed parameters\n",
    "        self.classifiers = {\n",
    "            'svc': get_classifier('svc'),\n",
    "            'logistic_regression': get_classifier('logistic_regression'),\n",
    "            'catboost': get_classifier('catboost')\n",
    "        }\n",
    "    \n",
    "    def _get_data_splits(self, embedding_file, random_state=42):\n",
    "        \"\"\"Prepare all data splits with 70-30 ratio\"\"\"\n",
    "        # Load embeddings\n",
    "        try:\n",
    "            self.emb = pd.read_csv(embedding_file)\n",
    "            print(f\"Loaded embeddings from {embedding_file}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Could not find {embedding_file}. Using a placeholder path.\")\n",
    "            self.emb = pd.read_csv(f'embeddings_output/{self.model_name}_PCA_150_components.csv')\n",
    "        \n",
    "        # CHANGE HERE: Use dataset instead of trying to extract labels from embeddings\n",
    "        # We need to load the 'dataset' that has the Label column\n",
    "        # Assuming a consistent filename for the dataset, but you may need to adjust this path\n",
    "        # try:\n",
    "        #     dataset = pd.read_csv('your_dataset.csv')  # Update with your actual dataset path\n",
    "        #     print(f\"Loaded dataset with labels\")\n",
    "        # except FileNotFoundError:\n",
    "        #     print(f\"Error: Could not find dataset with labels. Please provide the correct path.\")\n",
    "        #     return None\n",
    "        \n",
    "        # Use the labels from dataset\n",
    "        self.genuine_mask = dataset['Label'] == 0\n",
    "        self.fake_manual_mask = dataset['Label'] == 1\n",
    "        self.gptv1_mask = dataset['Label'].isin([10, 11])\n",
    "        self.gptv2_mask = dataset['Label'] == 12\n",
    "            \n",
    "        # Get generated profile embeddings file based on current model\n",
    "        generated_file = f'embeddings_output/{self.model_name}_PCA_150_components_generated.csv'\n",
    "        \n",
    "        # Split genuine profiles (70-30)\n",
    "        X_genuine = self.emb[self.genuine_mask]\n",
    "        X_train_genuine, X_test_genuine = train_test_split(\n",
    "            X_genuine, test_size=0.3, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Split manual fake profiles (70-30)\n",
    "        X_fake = self.emb[self.fake_manual_mask]\n",
    "        X_train_fake, X_test_fake = train_test_split(\n",
    "            X_fake, test_size=0.3, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        splits = {\n",
    "            'train': {\n",
    "                'genuine': X_train_genuine,\n",
    "                'fake_manual': X_train_fake,\n",
    "            },\n",
    "            'test': {\n",
    "                'genuine': X_test_genuine,\n",
    "                'fake_manual': X_test_fake,\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add GPTv1 data if available\n",
    "        if any(self.gptv1_mask):\n",
    "            X_gptv1 = self.emb[self.gptv1_mask]\n",
    "            X_train_gptv1, X_test_gptv1 = train_test_split(\n",
    "                X_gptv1, test_size=0.3, random_state=random_state\n",
    "            )\n",
    "            splits['train']['gptv1'] = X_train_gptv1\n",
    "            splits['test']['gptv1'] = X_test_gptv1\n",
    "        else:\n",
    "            # Use dummy data for GPTv1 if not available\n",
    "            print(\"Warning: No GPTv1 data found. Using empty DataFrame.\")\n",
    "            splits['train']['gptv1'] = pd.DataFrame()\n",
    "            splits['test']['gptv1'] = pd.DataFrame()\n",
    "            \n",
    "        # Load GPTv2 profiles if available\n",
    "        try:\n",
    "            gptv2_emb = pd.read_csv(generated_file)\n",
    "            # Only use the first 600 rows if available\n",
    "            if len(gptv2_emb) > 0:\n",
    "                gptv2_emb = gptv2_emb.iloc[:min(600, len(gptv2_emb))]\n",
    "                X_train_gptv2, X_test_gptv2 = train_test_split(\n",
    "                    gptv2_emb, test_size=0.3, random_state=random_state\n",
    "                )\n",
    "                splits['train']['gptv2'] = X_train_gptv2\n",
    "                splits['test']['gptv2'] = X_test_gptv2\n",
    "            else:\n",
    "                # Use dummy data for GPTv2 if file exists but is empty\n",
    "                print(\"Warning: GPTv2 file exists but has no data. Using empty DataFrame.\")\n",
    "                splits['train']['gptv2'] = pd.DataFrame()\n",
    "                splits['test']['gptv2'] = pd.DataFrame()\n",
    "        except FileNotFoundError:\n",
    "            # Use dummy data for GPTv2 if file doesn't exist\n",
    "            print(f\"Warning: {generated_file} not found. Using empty DataFrame for GPTv2.\")\n",
    "            splits['train']['gptv2'] = pd.DataFrame()\n",
    "            splits['test']['gptv2'] = pd.DataFrame()\n",
    "        \n",
    "        return splits\n",
    "        \n",
    "    def _get_training_data(self, train_data, scenario):\n",
    "        \"\"\"Get training data for each scenario\"\"\"\n",
    "        if scenario == 'baseline':\n",
    "            X_train = pd.concat([\n",
    "                train_data['genuine'],\n",
    "                train_data['fake_manual']\n",
    "            ])\n",
    "            y_train = pd.Series([0]*len(train_data['genuine']) + [1]*len(train_data['fake_manual']))\n",
    "            \n",
    "        elif scenario == 'gptv1_assisted':\n",
    "            if len(train_data['gptv1']) > 0:\n",
    "                X_train = pd.concat([\n",
    "                    train_data['genuine'],\n",
    "                    train_data['fake_manual'],\n",
    "                    train_data['gptv1']\n",
    "                ])\n",
    "                y_train = pd.Series([0]*len(train_data['genuine']) + \n",
    "                                   [1]*(len(train_data['fake_manual']) + len(train_data['gptv1'])))\n",
    "            else:\n",
    "                # Fall back to baseline if no GPTv1 data\n",
    "                print(\"Warning: No GPTv1 data. Falling back to baseline for training.\")\n",
    "                X_train = pd.concat([\n",
    "                    train_data['genuine'],\n",
    "                    train_data['fake_manual']\n",
    "                ])\n",
    "                y_train = pd.Series([0]*len(train_data['genuine']) + [1]*len(train_data['fake_manual']))\n",
    "            \n",
    "        elif scenario == 'gptv2_assisted':\n",
    "            if len(train_data['gptv2']) > 0:\n",
    "                X_train = pd.concat([\n",
    "                    train_data['genuine'],\n",
    "                    train_data['fake_manual'],\n",
    "                    train_data['gptv2']\n",
    "                ])\n",
    "                y_train = pd.Series([0]*len(train_data['genuine']) + \n",
    "                                   [1]*(len(train_data['fake_manual']) + len(train_data['gptv2'])))\n",
    "            else:\n",
    "                # Fall back to baseline if no GPTv2 data\n",
    "                print(\"Warning: No GPTv2 data. Falling back to baseline for training.\")\n",
    "                X_train = pd.concat([\n",
    "                    train_data['genuine'],\n",
    "                    train_data['fake_manual']\n",
    "                ])\n",
    "                y_train = pd.Series([0]*len(train_data['genuine']) + [1]*len(train_data['fake_manual']))\n",
    "            \n",
    "        elif scenario == 'gptv1v2_assisted':\n",
    "            # Check if both GPTv1 and GPTv2 data are available\n",
    "            if len(train_data['gptv1']) > 0 and len(train_data['gptv2']) > 0:\n",
    "                X_train = pd.concat([\n",
    "                    train_data['genuine'],\n",
    "                    train_data['fake_manual'],\n",
    "                    train_data['gptv1'],\n",
    "                    train_data['gptv2']\n",
    "                ])\n",
    "                y_train = pd.Series([0]*len(train_data['genuine']) + \n",
    "                                   [1]*(len(train_data['fake_manual']) + \n",
    "                                       len(train_data['gptv1']) + len(train_data['gptv2'])))\n",
    "            else:\n",
    "                # Fall back to baseline if either GPTv1 or GPTv2 data is missing\n",
    "                print(\"Warning: Missing GPTv1 or GPTv2 data. Falling back to baseline for training.\")\n",
    "                X_train = pd.concat([\n",
    "                    train_data['genuine'],\n",
    "                    train_data['fake_manual']\n",
    "                ])\n",
    "                y_train = pd.Series([0]*len(train_data['genuine']) + [1]*len(train_data['fake_manual']))\n",
    "        \n",
    "        # Scale the features\n",
    "        # Ensure X_train has data\n",
    "        if X_train.shape[0] > 0:\n",
    "            X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "            return X_train_scaled, y_train\n",
    "        else:\n",
    "            print(\"Error: No training data available.\")\n",
    "            return np.array([]), pd.Series([])\n",
    "    \n",
    "    def _get_test_data(self, test_data, scenario):\n",
    "        \"\"\"Get test data for each scenario\"\"\"\n",
    "        if scenario == 'baseline':\n",
    "            X_test = pd.concat([\n",
    "                test_data['genuine'],\n",
    "                test_data['fake_manual']\n",
    "            ]).reset_index(drop=True)\n",
    "            y_test = pd.Series([0]*len(test_data['genuine']) + [1]*len(test_data['fake_manual']))\n",
    "            \n",
    "        elif scenario == 'gptv1':\n",
    "            if len(test_data['gptv1']) > 0:\n",
    "                X_test = pd.concat([\n",
    "                    test_data['genuine'],\n",
    "                    test_data['fake_manual'],\n",
    "                    test_data['gptv1']\n",
    "                ]).reset_index(drop=True)\n",
    "                y_test = pd.Series([0]*len(test_data['genuine']) + \n",
    "                                  [1]*(len(test_data['fake_manual']) + len(test_data['gptv1'])))\n",
    "            else:\n",
    "                # Fall back to baseline if no GPTv1 data\n",
    "                print(\"Warning: No GPTv1 data. Falling back to baseline for testing.\")\n",
    "                X_test = pd.concat([\n",
    "                    test_data['genuine'],\n",
    "                    test_data['fake_manual']\n",
    "                ]).reset_index(drop=True)\n",
    "                y_test = pd.Series([0]*len(test_data['genuine']) + [1]*len(test_data['fake_manual']))\n",
    "            \n",
    "        elif scenario == 'gptv2':\n",
    "            if len(test_data['gptv2']) > 0:\n",
    "                X_test = pd.concat([\n",
    "                    test_data['genuine'],\n",
    "                    test_data['fake_manual'],\n",
    "                    test_data['gptv2']\n",
    "                ]).reset_index(drop=True)\n",
    "                y_test = pd.Series([0]*len(test_data['genuine']) + \n",
    "                                  [1]*(len(test_data['fake_manual']) + len(test_data['gptv2'])))\n",
    "            else:\n",
    "                # Fall back to baseline if no GPTv2 data\n",
    "                print(\"Warning: No GPTv2 data. Falling back to baseline for testing.\")\n",
    "                X_test = pd.concat([\n",
    "                    test_data['genuine'],\n",
    "                    test_data['fake_manual']\n",
    "                ]).reset_index(drop=True)\n",
    "                y_test = pd.Series([0]*len(test_data['genuine']) + [1]*len(test_data['fake_manual']))\n",
    "            \n",
    "        else:  # gptv1v2\n",
    "            # Check if both GPTv1 and GPTv2 data are available\n",
    "            if len(test_data['gptv1']) > 0 and len(test_data['gptv2']) > 0:\n",
    "                X_test = pd.concat([\n",
    "                    test_data['genuine'],\n",
    "                    test_data['fake_manual'],\n",
    "                    test_data['gptv1'],\n",
    "                    test_data['gptv2']\n",
    "                ]).reset_index(drop=True)\n",
    "                y_test = pd.Series([0]*len(test_data['genuine']) + \n",
    "                                  [1]*(len(test_data['fake_manual']) + \n",
    "                                      len(test_data['gptv1']) + len(test_data['gptv2'])))\n",
    "            else:\n",
    "                # Fall back to baseline if either GPTv1 or GPTv2 data is missing\n",
    "                print(\"Warning: Missing GPTv1 or GPTv2 data. Falling back to baseline for testing.\")\n",
    "                X_test = pd.concat([\n",
    "                    test_data['genuine'],\n",
    "                    test_data['fake_manual']\n",
    "                ]).reset_index(drop=True)\n",
    "                y_test = pd.Series([0]*len(test_data['genuine']) + [1]*len(test_data['fake_manual']))\n",
    "        \n",
    "        # Scale the features using the training scaler\n",
    "        # Ensure X_test has data\n",
    "        if X_test.shape[0] > 0:\n",
    "            X_test_scaled = self.scaler.transform(X_test)\n",
    "            return X_test_scaled, y_test\n",
    "        else:\n",
    "            print(\"Error: No test data available.\")\n",
    "            return np.array([]), pd.Series([])\n",
    "    \n",
    "    def evaluate_brier_vs_far(self, embedding_file):\n",
    "        \"\"\"Evaluate Brier score vs FAR relationship across scenarios and classifiers\"\"\"\n",
    "        splits = self._get_data_splits(embedding_file)\n",
    "        training_scenarios = ['baseline', 'gptv1_assisted', 'gptv2_assisted', 'gptv1v2_assisted']\n",
    "        test_scenarios = ['baseline', 'gptv1', 'gptv2', 'gptv1v2']\n",
    "        results = {}\n",
    "        \n",
    "        # For each classifier type\n",
    "        for clf_name, clf in self.classifiers.items():\n",
    "            print(f\"\\nEvaluating {clf_name} classifier with {self.model_name} embeddings...\")\n",
    "            results[clf_name] = {}\n",
    "            \n",
    "            # For each training scenario\n",
    "            for train_scenario in training_scenarios:\n",
    "                print(f\"  Training scenario: {train_scenario}\")\n",
    "                X_train, y_train = self._get_training_data(splits['train'], train_scenario)\n",
    "                \n",
    "                # Skip if training data is empty\n",
    "                if X_train.shape[0] == 0 or len(y_train) == 0:\n",
    "                    print(f\"  Skipping {train_scenario} due to empty training data.\")\n",
    "                    continue\n",
    "                \n",
    "                # Train the model\n",
    "                clf.fit(X_train, y_train)\n",
    "                \n",
    "                # Store results for each test scenario\n",
    "                results[clf_name][train_scenario] = {}\n",
    "                \n",
    "                # For each test scenario\n",
    "                for test_scenario in test_scenarios:\n",
    "                    print(f\"    Testing against: {test_scenario}\")\n",
    "                    X_test, y_test = self._get_test_data(splits['test'], test_scenario)\n",
    "                    \n",
    "                    # Skip if test data is empty\n",
    "                    if X_test.shape[0] == 0 or len(y_test) == 0:\n",
    "                        print(f\"    Skipping {test_scenario} due to empty test data.\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Get predictions and probabilities\n",
    "                    y_pred = clf.predict(X_test)\n",
    "                    \n",
    "                    # Get probabilities for class 1 (fake)\n",
    "                    y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    metrics = calculate_metrics(y_test, y_pred, y_prob)\n",
    "                    results[clf_name][train_scenario][test_scenario] = metrics\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def export_results_to_csv(self, results):\n",
    "        \"\"\"Export results to CSV for analysis\"\"\"\n",
    "        rows = []\n",
    "        for clf_name in results.keys():\n",
    "            for train_scenario in results[clf_name].keys():\n",
    "                for test_scenario in results[clf_name][train_scenario].keys():\n",
    "                    metrics = results[clf_name][train_scenario][test_scenario]\n",
    "                    row = {\n",
    "                        'model': self.model_name,\n",
    "                        'classifier': clf_name,\n",
    "                        'train_scenario': train_scenario,\n",
    "                        'test_scenario': test_scenario,\n",
    "                        'brier_score': metrics.get('brier_score', float('nan')),\n",
    "                        'far': metrics.get('far', float('nan')),\n",
    "                        'frr': metrics.get('frr', float('nan')),\n",
    "                        'f1_score': metrics.get('f1_score', float('nan'))\n",
    "                    }\n",
    "                    rows.append(row)\n",
    "        \n",
    "        df = pd.DataFrame(rows)\n",
    "        csv_path = f'{self.results_dir}/{self.model_name}_brier_far_results.csv'\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Exported results to {csv_path}\")\n",
    "        return df\n",
    "    \n",
    "    def calculate_correlations(self, df):\n",
    "        \"\"\"Calculate correlations between Brier score and FAR\"\"\"\n",
    "        correlations = []\n",
    "        \n",
    "        for clf_name in df['classifier'].unique():\n",
    "            clf_data = df[df['classifier'] == clf_name]\n",
    "            \n",
    "            # Calculate Pearson correlation\n",
    "            pearson_corr, p_value_pearson = pearsonr(\n",
    "                clf_data['brier_score'].values, \n",
    "                clf_data['far'].values\n",
    "            )\n",
    "            \n",
    "            # Calculate Spearman correlation\n",
    "            spearman_corr, p_value_spearman = spearmanr(\n",
    "                clf_data['brier_score'].values, \n",
    "                clf_data['far'].values\n",
    "            )\n",
    "            \n",
    "            correlations.append({\n",
    "                'model': self.model_name,\n",
    "                'classifier': clf_name,\n",
    "                'pearson_correlation': pearson_corr,\n",
    "                'pearson_p_value': p_value_pearson,\n",
    "            })\n",
    "        \n",
    "        corr_df = pd.DataFrame(correlations)\n",
    "        corr_csv_path = f'{self.results_dir}/{self.model_name}_brier_far_correlations.csv'\n",
    "        corr_df.to_csv(corr_csv_path, index=False)\n",
    "        print(f\"Exported correlation results to {corr_csv_path}\")\n",
    "        return corr_df\n",
    "    \n",
    "    def plot_brier_vs_far(self, df, separate_classifiers=True):\n",
    "        \"\"\"Create scatter plots of Brier score vs FAR with optimized fonts for 3x2 inch plots\"\"\"\n",
    "        if separate_classifiers:\n",
    "            # Create separate plots for each classifier\n",
    "            for clf_name in df['classifier'].unique():\n",
    "                clf_data = df[df['classifier'] == clf_name]\n",
    "                \n",
    "                # Set figure size to exactly 3x2 inches\n",
    "                plt.figure(figsize=(3, 2), dpi=300)\n",
    "                \n",
    "                # Create scatter plot with different colors for each training scenario\n",
    "                # Use smaller markers due to the small plot size\n",
    "                for train_scenario in clf_data['train_scenario'].unique():\n",
    "                    train_data = clf_data[clf_data['train_scenario'] == train_scenario]\n",
    "                    \n",
    "                    # Shorten training scenario labels to save space\n",
    "                    scenario_label = train_scenario\n",
    "                \n",
    "                    plt.scatter(\n",
    "                        train_data['brier_score'], \n",
    "                        train_data['far'],\n",
    "                        label=scenario_label,\n",
    "                        alpha=0.7,\n",
    "                        s=20  # Smaller point size\n",
    "                    )\n",
    "                \n",
    "                # Add labels and title with appropriate font sizes\n",
    "                plt.xlabel('Brier Score', fontsize=7)\n",
    "                plt.ylabel('FAR', fontsize=7)  # Shortened label\n",
    "                \n",
    "                # Use a shorter classifier name in the title\n",
    "                clf_short = clf_name.replace('logistic_regression', 'LogReg').replace('catboost', 'CatB')\n",
    "                plt.title(f'{self.model_name} - {clf_short}', fontsize=8)\n",
    "                \n",
    "                # Configure tick labels to be smaller\n",
    "                plt.tick_params(axis='both', which='major', labelsize=6)\n",
    "                \n",
    "                # Use a tight grid with smaller lines\n",
    "                plt.grid(True, linestyle=':', alpha=0.5, linewidth=0.5)\n",
    "                \n",
    "                # Make the legend smaller and position it optimally\n",
    "                plt.legend(fontsize=4, loc='best', framealpha=0.7, \n",
    "                          markerscale=0.8, handlelength=1, handletextpad=0.5)\n",
    "                \n",
    "                # Add correlation line\n",
    "                x = clf_data['brier_score'].values\n",
    "                y = clf_data['far'].values\n",
    "                \n",
    "                # Use polyfit to get line of best fit\n",
    "                m, b = np.polyfit(x, y, 1)\n",
    "                plt.plot(x, m*x + b, 'r--', linewidth=0.7)\n",
    "                \n",
    "                # Calculate Pearson correlation\n",
    "                pearson_corr, _ = pearsonr(x, y)\n",
    "                # Add correlation in the corner with smaller font\n",
    "                plt.annotate(f'r = {pearson_corr:.2f}', \n",
    "                            xy=(0.05, 0.90), \n",
    "                            xycoords='axes fraction',\n",
    "                            fontsize=6,\n",
    "                            bbox=dict(boxstyle=\"round,pad=0.1\", fc=\"white\", ec=\"gray\", alpha=0.7))\n",
    "                \n",
    "                # Use tight layout to maximize plot area\n",
    "                plt.tight_layout(pad=0.1)\n",
    "                \n",
    "                # Save the plot at high resolution\n",
    "                plt_path = f'{self.results_dir}/{self.model_name}_{clf_name}_brier_vs_far.png'\n",
    "                plt.savefig(plt_path, dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                print(f\"Saved plot to {plt_path}\")\n",
    "        \n",
    "        # Create a combined plot with all classifiers (keeping this at the original larger size)\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        \n",
    "        # Create scatter plot with different colors for each classifier\n",
    "        for clf_name in df['classifier'].unique():\n",
    "            clf_data = df[df['classifier'] == clf_name]\n",
    "            \n",
    "            # Shorten classifier names\n",
    "            clf_short = clf_name.replace('logistic_regression', 'LogReg').replace('catboost', 'CatB')\n",
    "            \n",
    "            plt.scatter(\n",
    "                clf_data['brier_score'], \n",
    "                clf_data['far'],\n",
    "                label=clf_short,\n",
    "                alpha=0.7,\n",
    "                s=30\n",
    "            )\n",
    "        \n",
    "        # Add labels and title\n",
    "        plt.xlabel('Brier Score', fontsize=9)\n",
    "        plt.ylabel('False Accept Rate (FAR)', fontsize=9)\n",
    "        plt.title(f'Brier Score vs FAR - {self.model_name}', fontsize=10)\n",
    "        plt.tick_params(axis='both', which='major', labelsize=8)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7, linewidth=0.5)\n",
    "        plt.legend(fontsize=6)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        plt_path = f'{self.results_dir}/{self.model_name}_all_classifiers_brier_vs_far.png'\n",
    "        plt.savefig(plt_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved combined plot to {plt_path}\")\n",
    "    \n",
    "    def plot_heatmap(self, df):\n",
    "        \"\"\"Create a heatmap of Brier score vs attack scenario\"\"\"\n",
    "        # Create a pivot table for the heatmap\n",
    "        pivot_brier = df.pivot_table(\n",
    "            index=['classifier', 'train_scenario'],\n",
    "            columns='test_scenario',\n",
    "            values='brier_score'\n",
    "        )\n",
    "        \n",
    "        pivot_far = df.pivot_table(\n",
    "            index=['classifier', 'train_scenario'],\n",
    "            columns='test_scenario',\n",
    "            values='far'\n",
    "        )\n",
    "        \n",
    "        # Create heatmap for Brier scores\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(pivot_brier, annot=True, cmap='YlOrRd', fmt='.4f')\n",
    "        plt.title(f'Brier Score Heatmap - {self.model_name}', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        plt_path = f'{self.results_dir}/{self.model_name}_brier_heatmap.png'\n",
    "        plt.savefig(plt_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved Brier score heatmap to {plt_path}\")\n",
    "        \n",
    "        # Create heatmap for FAR\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(pivot_far, annot=True, cmap='YlOrRd', fmt='.4f')\n",
    "        plt.title(f'FAR Heatmap - {self.model_name}', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        plt_path = f'{self.results_dir}/{self.model_name}_far_heatmap.png'\n",
    "        plt.savefig(plt_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved FAR heatmap to {plt_path}\")\n",
    "\n",
    "def main():\n",
    "    # Models to analyze\n",
    "    models = ['roberta', 'modernbert', 'deberta', 'flair']\n",
    "    \n",
    "    # Container for all results\n",
    "    all_results = {}\n",
    "    all_dfs = []\n",
    "    \n",
    "    # Analyze each model\n",
    "    for model_name in models:\n",
    "        print(f\"\\n{'='*30} ANALYZING {model_name.upper()} {'='*30}\")\n",
    "        \n",
    "        # Initialize analyzer\n",
    "        analyzer = BrierFARAnalysis(model_name)\n",
    "        \n",
    "        # Embedding file path\n",
    "        embedding_file = f'embeddings_output/{model_name}_PCA_150_components.csv'\n",
    "        \n",
    "        # Evaluate Brier vs FAR\n",
    "        results = analyzer.evaluate_brier_vs_far(embedding_file)\n",
    "        all_results[model_name] = results\n",
    "        \n",
    "        # Export results to CSV\n",
    "        df = analyzer.export_results_to_csv(results)\n",
    "        all_dfs.append(df)\n",
    "        \n",
    "        # Calculate correlations\n",
    "        corr_df = analyzer.calculate_correlations(df)\n",
    "        \n",
    "        # Create plots\n",
    "        analyzer.plot_brier_vs_far(df)\n",
    "        analyzer.plot_heatmap(df)\n",
    "    \n",
    "    # Combine all results\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    combined_csv_path = 'brier_far_analysis/combined_brier_far_results.csv'\n",
    "    combined_df.to_csv(combined_csv_path, index=False)\n",
    "    print(f\"Exported combined results to {combined_csv_path}\")\n",
    "    \n",
    "    # Create combined correlation analysis\n",
    "    combined_corrs = []\n",
    "    for model_name in models:\n",
    "        model_df = combined_df[combined_df['model'] == model_name]\n",
    "        for clf_name in model_df['classifier'].unique():\n",
    "            clf_data = model_df[model_df['classifier'] == clf_name]\n",
    "            \n",
    "            # Calculate Pearson correlation\n",
    "            pearson_corr, p_value_pearson = pearsonr(\n",
    "                clf_data['brier_score'].values, \n",
    "                clf_data['far'].values\n",
    "            )\n",
    "            \n",
    "            # Calculate Spearman correlation\n",
    "            spearman_corr, p_value_spearman = spearmanr(\n",
    "                clf_data['brier_score'].values, \n",
    "                clf_data['far'].values\n",
    "            )\n",
    "            \n",
    "            combined_corrs.append({\n",
    "                'model': model_name,\n",
    "                'classifier': clf_name,\n",
    "                'pearson_correlation': pearson_corr,\n",
    "                'pearson_p_value': p_value_pearson,\n",
    "            })\n",
    "    \n",
    "    corr_df = pd.DataFrame(combined_corrs)\n",
    "    corr_csv_path = 'brier_far_analysis/combined_brier_far_correlations.csv'\n",
    "    corr_df.to_csv(corr_csv_path, index=False)\n",
    "    print(f\"Exported combined correlation results to {corr_csv_path}\")\n",
    "    \n",
    "    # Create a summary plot for all models and classifiers\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Colors for different models and markers for different classifiers\n",
    "    model_colors = {'roberta': 'blue', 'modernbert': 'green', 'deberta': 'red', 'flair': 'purple'}\n",
    "    clf_markers = {'svc': 'o', 'logistic_regression': 's', 'catboost': '^'}\n",
    "    \n",
    "    for model_name in models:\n",
    "        model_df = combined_df[combined_df['model'] == model_name]\n",
    "        for clf_name in model_df['classifier'].unique():\n",
    "            clf_data = model_df[model_df['classifier'] == clf_name]\n",
    "            \n",
    "            plt.scatter(\n",
    "                clf_data['brier_score'], \n",
    "                clf_data['far'],\n",
    "                label=f\"{model_name} - {clf_name}\",\n",
    "                color=model_colors.get(model_name, 'black'),\n",
    "                marker=clf_markers.get(clf_name, 'x'),\n",
    "                alpha=0.7,\n",
    "                s=80\n",
    "            )\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Brier Score', fontsize=14)\n",
    "    plt.ylabel('False Accept Rate (FAR)', fontsize=14)\n",
    "    plt.title('Brier Score vs FAR - All Models and Classifiers', fontsize=16)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Save the plot\n",
    "    plt_path = 'brier_far_analysis/all_models_classifiers_brier_vs_far.png'\n",
    "    plt.savefig(plt_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved combined plot for all models to {plt_path}\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e36c94-b506-48a4-ac1c-a6d4f3664e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
