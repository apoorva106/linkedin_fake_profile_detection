{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBhHW4vHSWCc",
        "outputId": "a7a457b2-c01e-4183-83d9-cc8545744e23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Aggregated Human Benchmarking Results\n",
            "============================================================\n",
            "Number of benchmarks analyzed: 30\n",
            "Total samples across all benchmarks: 450\n",
            "\n",
            "Overall Accuracy:\n",
            "Mean: 41.56% ± 16.03%\n",
            "Range: [13.33%, 80.00%]\n",
            "\n",
            "Metrics by Category:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Legitimate Profiles:\n",
            "PRECISION:\n",
            "  Mean: 39.74% ± 21.88%\n",
            "  Range: [0.00%, 80.00%]\n",
            "RECALL:\n",
            "  Mean: 53.10% ± 32.30%\n",
            "  Range: [0.00%, 100.00%]\n",
            "FAR:\n",
            "  Mean: 38.63% ± 18.80%\n",
            "  Range: [9.09%, 77.78%]\n",
            "FRR:\n",
            "  Mean: 46.90% ± 32.30%\n",
            "  Range: [0.00%, 100.00%]\n",
            "F1:\n",
            "  Mean: 43.79% ± 24.70%\n",
            "  Range: [0.00%, 83.33%]\n",
            "\n",
            "Manual Fake Profiles:\n",
            "PRECISION:\n",
            "  Mean: 44.86% ± 21.00%\n",
            "  Range: [0.00%, 100.00%]\n",
            "RECALL:\n",
            "  Mean: 40.10% ± 21.56%\n",
            "  Range: [0.00%, 100.00%]\n",
            "FAR:\n",
            "  Mean: 24.14% ± 12.86%\n",
            "  Range: [0.00%, 50.00%]\n",
            "FRR:\n",
            "  Mean: 59.90% ± 21.56%\n",
            "  Range: [0.00%, 100.00%]\n",
            "F1:\n",
            "  Mean: 41.17% ± 19.66%\n",
            "  Range: [0.00%, 80.00%]\n",
            "\n",
            "Gpt Profiles:\n",
            "PRECISION:\n",
            "  Mean: 36.69% ± 29.47%\n",
            "  Range: [0.00%, 100.00%]\n",
            "RECALL:\n",
            "  Mean: 31.22% ± 26.73%\n",
            "  Range: [0.00%, 100.00%]\n",
            "FAR:\n",
            "  Mean: 24.99% ± 14.27%\n",
            "  Range: [0.00%, 50.00%]\n",
            "FRR:\n",
            "  Mean: 68.78% ± 26.73%\n",
            "  Range: [0.00%, 100.00%]\n",
            "F1:\n",
            "  Mean: 32.41% ± 25.65%\n",
            "  Range: [0.00%, 88.89%]\n",
            "\n",
            "Average Confusion Matrix:\n",
            "------------------------------------------------------------\n",
            "Actual vs Predicted (legitimate, manual-fake, gpt)\n",
            "['2.67', '0.90', '1.43']\n",
            "['1.93', '2.00', '1.07']\n",
            "['1.93', '1.50', '1.57']\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from typing import List, Dict\n",
        "import pandas as pd\n",
        "\n",
        "class HumanBenchmarkAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.all_results = []\n",
        "        self.aggregated_results = None\n",
        "\n",
        "    def load_json_files(self, directory_path: str) -> None:\n",
        "        \"\"\"Load all JSON files from the specified directory.\"\"\"\n",
        "        json_files = [f for f in os.listdir(directory_path) if f.endswith('.json')]\n",
        "\n",
        "        for file_name in json_files:\n",
        "            file_path = os.path.join(directory_path, file_name)\n",
        "            try:\n",
        "                with open(file_path, 'r') as f:\n",
        "                    responses = json.load(f)\n",
        "                result = self.analyze_single_benchmark(responses, file_name)\n",
        "                self.all_results.append(result)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_name}: {str(e)}\")\n",
        "\n",
        "    def analyze_single_benchmark(self, responses: List[Dict], file_name: str) -> Dict:\n",
        "        \"\"\"Analyze a single benchmark response set.\"\"\"\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "\n",
        "        # Statistics for each category\n",
        "        stats = {\n",
        "            'legitimate': {'TP': 0, 'FP': 0, 'TN': 0, 'FN': 0},\n",
        "            'manual-fake': {'TP': 0, 'FP': 0, 'TN': 0, 'FN': 0},\n",
        "            'gpt': {'TP': 0, 'FP': 0, 'TN': 0, 'FN': 0}\n",
        "        }\n",
        "\n",
        "        for response in responses:\n",
        "            actual = response['actualType']\n",
        "            predicted = response['userResponse']\n",
        "\n",
        "            y_true.append(actual)\n",
        "            y_pred.append(predicted)\n",
        "\n",
        "            # Update statistics for each category\n",
        "            for category in stats.keys():\n",
        "                is_actual = (actual == category)\n",
        "                is_predicted = (predicted == category)\n",
        "\n",
        "                if is_actual and is_predicted:\n",
        "                    stats[category]['TP'] += 1\n",
        "                elif not is_actual and is_predicted:\n",
        "                    stats[category]['FP'] += 1\n",
        "                elif is_actual and not is_predicted:\n",
        "                    stats[category]['FN'] += 1\n",
        "                else:\n",
        "                    stats[category]['TN'] += 1\n",
        "\n",
        "        # Calculate metrics\n",
        "        def calculate_metrics(stat):\n",
        "            precision = stat['TP'] / (stat['TP'] + stat['FP']) if (stat['TP'] + stat['FP']) > 0 else 0\n",
        "            recall = stat['TP'] / (stat['TP'] + stat['FN']) if (stat['TP'] + stat['FN']) > 0 else 0\n",
        "            far = stat['FP'] / (stat['FP'] + stat['TN']) if (stat['FP'] + stat['TN']) > 0 else 0\n",
        "            frr = stat['FN'] / (stat['TP'] + stat['FN']) if (stat['TP'] + stat['FN']) > 0 else 0\n",
        "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "            return {\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'FAR': far,\n",
        "                'FRR': frr,\n",
        "                'F1': f1\n",
        "            }\n",
        "\n",
        "        # Calculate confusion matrix\n",
        "        labels = ['legitimate', 'manual-fake', 'gpt']\n",
        "        conf_matrix = confusion_matrix(y_true, y_pred, labels=labels).tolist()\n",
        "\n",
        "        return {\n",
        "            'file_name': file_name,\n",
        "            'overall_accuracy': accuracy_score(y_true, y_pred),\n",
        "            'total_samples': len(responses),\n",
        "            'confusion_matrix': conf_matrix,\n",
        "            'legitimate': calculate_metrics(stats['legitimate']),\n",
        "            'manual_fake': calculate_metrics(stats['manual-fake']),\n",
        "            'gpt': calculate_metrics(stats['gpt'])\n",
        "        }\n",
        "\n",
        "    def aggregate_results(self) -> Dict:\n",
        "        \"\"\"Aggregate results across all benchmarks.\"\"\"\n",
        "        if not self.all_results:\n",
        "            return None\n",
        "\n",
        "        # Initialize aggregated metrics\n",
        "        agg_metrics = {\n",
        "            'overall_accuracy': [],\n",
        "            'legitimate': {'precision': [], 'recall': [], 'FAR': [], 'FRR': [], 'F1': []},\n",
        "            'manual_fake': {'precision': [], 'recall': [], 'FAR': [], 'FRR': [], 'F1': []},\n",
        "            'gpt': {'precision': [], 'recall': [], 'FAR': [], 'FRR': [], 'F1': []},\n",
        "            'total_samples': 0,\n",
        "            'confusion_matrices': []\n",
        "        }\n",
        "\n",
        "        # Collect metrics from all results\n",
        "        for result in self.all_results:\n",
        "            agg_metrics['overall_accuracy'].append(result['overall_accuracy'])\n",
        "            agg_metrics['total_samples'] += result['total_samples']\n",
        "            agg_metrics['confusion_matrices'].append(result['confusion_matrix'])\n",
        "\n",
        "            for category in ['legitimate', 'manual_fake', 'gpt']:\n",
        "                for metric in ['precision', 'recall', 'FAR', 'FRR', 'F1']:\n",
        "                    agg_metrics[category][metric].append(result[category][metric])\n",
        "\n",
        "        # Calculate summary statistics\n",
        "        self.aggregated_results = {\n",
        "            'number_of_benchmarks': len(self.all_results),\n",
        "            'total_samples': agg_metrics['total_samples'],\n",
        "            'overall_accuracy': {\n",
        "                'mean': np.mean(agg_metrics['overall_accuracy']),\n",
        "                'std': np.std(agg_metrics['overall_accuracy']),\n",
        "                'min': np.min(agg_metrics['overall_accuracy']),\n",
        "                'max': np.max(agg_metrics['overall_accuracy'])\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Calculate category-specific statistics\n",
        "        for category in ['legitimate', 'manual_fake', 'gpt']:\n",
        "            self.aggregated_results[category] = {}\n",
        "            for metric in ['precision', 'recall', 'FAR', 'FRR', 'F1']:\n",
        "                values = agg_metrics[category][metric]\n",
        "                self.aggregated_results[category][metric] = {\n",
        "                    'mean': np.mean(values),\n",
        "                    'std': np.std(values),\n",
        "                    'min': np.min(values),\n",
        "                    'max': np.max(values)\n",
        "                }\n",
        "\n",
        "        # Calculate average confusion matrix\n",
        "        self.aggregated_results['average_confusion_matrix'] = np.mean(\n",
        "            agg_metrics['confusion_matrices'], axis=0).tolist()\n",
        "\n",
        "        return self.aggregated_results\n",
        "\n",
        "    def print_detailed_results(self) -> None:\n",
        "        \"\"\"Print detailed analysis results.\"\"\"\n",
        "        if not self.aggregated_results:\n",
        "            print(\"No results to display. Please run analysis first.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\nAggregated Human Benchmarking Results\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Number of benchmarks analyzed: {self.aggregated_results['number_of_benchmarks']}\")\n",
        "        print(f\"Total samples across all benchmarks: {self.aggregated_results['total_samples']}\")\n",
        "\n",
        "        print(\"\\nOverall Accuracy:\")\n",
        "        acc = self.aggregated_results['overall_accuracy']\n",
        "        print(f\"Mean: {acc['mean']:.2%} ± {acc['std']:.2%}\")\n",
        "        print(f\"Range: [{acc['min']:.2%}, {acc['max']:.2%}]\")\n",
        "\n",
        "        print(\"\\nMetrics by Category:\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        categories = ['legitimate', 'manual_fake', 'gpt']\n",
        "        metrics = ['precision', 'recall', 'FAR', 'FRR', 'F1']\n",
        "\n",
        "        for category in categories:\n",
        "            print(f\"\\n{category.replace('_', ' ').title()} Profiles:\")\n",
        "            for metric in metrics:\n",
        "                stats = self.aggregated_results[category][metric]\n",
        "                print(f\"{metric.upper()}:\")\n",
        "                print(f\"  Mean: {stats['mean']:.2%} ± {stats['std']:.2%}\")\n",
        "                print(f\"  Range: [{stats['min']:.2%}, {stats['max']:.2%}]\")\n",
        "\n",
        "        print(\"\\nAverage Confusion Matrix:\")\n",
        "        print(\"-\" * 60)\n",
        "        print(\"Actual vs Predicted (legitimate, manual-fake, gpt)\")\n",
        "        for row in self.aggregated_results['average_confusion_matrix']:\n",
        "            print([f\"{x:.2f}\" for x in row])\n",
        "\n",
        "    def export_results_to_csv(self, output_path: str) -> None:\n",
        "        \"\"\"Export detailed results to CSV files.\"\"\"\n",
        "        if not self.aggregated_results:\n",
        "            print(\"No results to export. Please run analysis first.\")\n",
        "            return\n",
        "\n",
        "        # Create detailed results dataframe\n",
        "        detailed_results = []\n",
        "        for result in self.all_results:\n",
        "            row = {\n",
        "                'file_name': result['file_name'],\n",
        "                'accuracy': result['overall_accuracy'],\n",
        "                'total_samples': result['total_samples']\n",
        "            }\n",
        "\n",
        "            # Add category-specific metrics\n",
        "            for category in ['legitimate', 'manual_fake', 'gpt']:\n",
        "                for metric in ['precision', 'recall', 'FAR', 'FRR', 'F1']:\n",
        "                    row[f\"{category}_{metric}\"] = result[category][metric]\n",
        "\n",
        "            detailed_results.append(row)\n",
        "\n",
        "        # Create summary dataframe\n",
        "        summary_data = []\n",
        "        metrics = ['mean', 'std', 'min', 'max']\n",
        "\n",
        "        # Add overall accuracy\n",
        "        for metric in metrics:\n",
        "            row = {'metric': metric, 'overall_accuracy': self.aggregated_results['overall_accuracy'][metric]}\n",
        "\n",
        "            # Add category-specific metrics\n",
        "            for category in ['legitimate', 'manual_fake', 'gpt']:\n",
        "                for measure in ['precision', 'recall', 'FAR', 'FRR', 'F1']:\n",
        "                    row[f\"{category}_{measure}\"] = self.aggregated_results[category][measure][metric]\n",
        "\n",
        "            summary_data.append(row)\n",
        "\n",
        "        # Export to CSV\n",
        "        pd.DataFrame(detailed_results).to_csv(f\"{output_path}_detailed.csv\", index=False)\n",
        "        pd.DataFrame(summary_data).to_csv(f\"{output_path}_summary.csv\", index=False)\n",
        "\n",
        "# Example usage:\n",
        "analyzer = HumanBenchmarkAnalyzer()\n",
        "analyzer.load_json_files('survey_responses')  # Directory containing JSON files\n",
        "analyzer.aggregate_results()\n",
        "analyzer.print_detailed_results()\n",
        "analyzer.export_results_to_csv('human_benchmark_results')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from typing import List, Dict\n",
        "import pandas as pd\n",
        "\n",
        "class BinaryBenchmarkAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.all_results = []\n",
        "        self.aggregated_results = None\n",
        "\n",
        "    def load_json_files(self, directory_path: str) -> None:\n",
        "        json_files = [f for f in os.listdir(directory_path) if f.endswith('.json')]\n",
        "\n",
        "        for file_name in json_files:\n",
        "            file_path = os.path.join(directory_path, file_name)\n",
        "            try:\n",
        "                with open(file_path, 'r') as f:\n",
        "                    responses = json.load(f)\n",
        "                result = self.analyze_single_benchmark(responses, file_name)\n",
        "                self.all_results.append(result)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_name}: {str(e)}\")\n",
        "\n",
        "    def analyze_single_benchmark(self, responses: List[Dict], file_name: str) -> Dict:\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "\n",
        "        # Statistics for binary classification\n",
        "        stats = {\n",
        "            'legitimate': {'TP': 0, 'FP': 0, 'TN': 0, 'FN': 0}\n",
        "        }\n",
        "\n",
        "        for response in responses:\n",
        "            # Convert actual type to binary (legitimate or fake)\n",
        "            actual = 'legitimate' if response['actualType'] == 'legitimate' else 'fake'\n",
        "\n",
        "            # Convert predicted type to binary\n",
        "            predicted = 'legitimate' if response['userResponse'] == 'legitimate' else 'fake'\n",
        "\n",
        "            y_true.append(actual)\n",
        "            y_pred.append(predicted)\n",
        "\n",
        "            # Update binary statistics\n",
        "            is_actual_legitimate = (actual == 'legitimate')\n",
        "            is_predicted_legitimate = (predicted == 'legitimate')\n",
        "\n",
        "            if is_actual_legitimate and is_predicted_legitimate:\n",
        "                stats['legitimate']['TP'] += 1\n",
        "            elif not is_actual_legitimate and is_predicted_legitimate:\n",
        "                stats['legitimate']['FP'] += 1\n",
        "            elif is_actual_legitimate and not is_predicted_legitimate:\n",
        "                stats['legitimate']['FN'] += 1\n",
        "            else:\n",
        "                stats['legitimate']['TN'] += 1\n",
        "\n",
        "        # Calculate metrics\n",
        "        def calculate_metrics(stat):\n",
        "            precision = stat['TP'] / (stat['TP'] + stat['FP']) if (stat['TP'] + stat['FP']) > 0 else 0\n",
        "            recall = stat['TP'] / (stat['TP'] + stat['FN']) if (stat['TP'] + stat['FN']) > 0 else 0\n",
        "            far = stat['FP'] / (stat['FP'] + stat['TN']) if (stat['FP'] + stat['TN']) > 0 else 0\n",
        "            frr = stat['FN'] / (stat['TP'] + stat['FN']) if (stat['TP'] + stat['FN']) > 0 else 0\n",
        "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "            return {\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'FAR': far,\n",
        "                'FRR': frr,\n",
        "                'F1': f1\n",
        "            }\n",
        "\n",
        "        # Calculate binary confusion matrix\n",
        "        conf_matrix = confusion_matrix(y_true, y_pred, labels=['legitimate', 'fake']).tolist()\n",
        "\n",
        "        return {\n",
        "            'file_name': file_name,\n",
        "            'overall_accuracy': accuracy_score(y_true, y_pred),\n",
        "            'total_samples': len(responses),\n",
        "            'confusion_matrix': conf_matrix,\n",
        "            'metrics': calculate_metrics(stats['legitimate'])\n",
        "        }\n",
        "\n",
        "    def aggregate_results(self) -> Dict:\n",
        "        if not self.all_results:\n",
        "            return None\n",
        "\n",
        "        # Initialize aggregated metrics\n",
        "        agg_metrics = {\n",
        "            'overall_accuracy': [],\n",
        "            'metrics': {\n",
        "                'precision': [], 'recall': [], 'FAR': [], 'FRR': [], 'F1': []\n",
        "            },\n",
        "            'total_samples': 0,\n",
        "            'confusion_matrices': []\n",
        "        }\n",
        "\n",
        "        # Collect metrics from all results\n",
        "        for result in self.all_results:\n",
        "            agg_metrics['overall_accuracy'].append(result['overall_accuracy'])\n",
        "            agg_metrics['total_samples'] += result['total_samples']\n",
        "            agg_metrics['confusion_matrices'].append(result['confusion_matrix'])\n",
        "\n",
        "            for metric in ['precision', 'recall', 'FAR', 'FRR', 'F1']:\n",
        "                agg_metrics['metrics'][metric].append(result['metrics'][metric])\n",
        "\n",
        "        # Calculate summary statistics\n",
        "        self.aggregated_results = {\n",
        "            'number_of_benchmarks': len(self.all_results),\n",
        "            'total_samples': agg_metrics['total_samples'],\n",
        "            'overall_accuracy': {\n",
        "                'mean': np.mean(agg_metrics['overall_accuracy']),\n",
        "                'std': np.std(agg_metrics['overall_accuracy']),\n",
        "                'min': np.min(agg_metrics['overall_accuracy']),\n",
        "                'max': np.max(agg_metrics['overall_accuracy'])\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Calculate metrics statistics\n",
        "        self.aggregated_results['metrics'] = {}\n",
        "        for metric in ['precision', 'recall', 'FAR', 'FRR', 'F1']:\n",
        "            values = agg_metrics['metrics'][metric]\n",
        "            self.aggregated_results['metrics'][metric] = {\n",
        "                'mean': np.mean(values),\n",
        "                'std': np.std(values),\n",
        "                'min': np.min(values),\n",
        "                'max': np.max(values)\n",
        "            }\n",
        "\n",
        "        # Calculate average confusion matrix\n",
        "        self.aggregated_results['average_confusion_matrix'] = np.mean(\n",
        "            agg_metrics['confusion_matrices'], axis=0).tolist()\n",
        "\n",
        "        return self.aggregated_results\n",
        "\n",
        "    def print_detailed_results(self) -> None:\n",
        "        if not self.aggregated_results:\n",
        "            print(\"No results to display. Please run analysis first.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\nAggregated Binary Classification Results\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Number of benchmarks analyzed: {self.aggregated_results['number_of_benchmarks']}\")\n",
        "        print(f\"Total samples across all benchmarks: {self.aggregated_results['total_samples']}\")\n",
        "\n",
        "        print(\"\\nOverall Accuracy:\")\n",
        "        acc = self.aggregated_results['overall_accuracy']\n",
        "        print(f\"Mean: {acc['mean']:.2%} ± {acc['std']:.2%}\")\n",
        "        print(f\"Range: [{acc['min']:.2%}, {acc['max']:.2%}]\")\n",
        "\n",
        "        print(\"\\nClassification Metrics:\")\n",
        "        print(\"-\" * 60)\n",
        "        metrics = ['precision', 'recall', 'FAR', 'FRR', 'F1']\n",
        "\n",
        "        for metric in metrics:\n",
        "            stats = self.aggregated_results['metrics'][metric]\n",
        "            print(f\"{metric.upper()}:\")\n",
        "            print(f\"  Mean: {stats['mean']:.2%} ± {stats['std']:.2%}\")\n",
        "            print(f\"  Range: [{stats['min']:.2%}, {stats['max']:.2%}]\")\n",
        "\n",
        "        print(\"\\nAverage Confusion Matrix:\")\n",
        "        print(\"-\" * 60)\n",
        "        print(\"Actual vs Predicted (legitimate, fake)\")\n",
        "        for row in self.aggregated_results['average_confusion_matrix']:\n",
        "            print([f\"{x:.2f}\" for x in row])\n",
        "\n",
        "    def export_results_to_csv(self, output_path: str) -> None:\n",
        "        if not self.aggregated_results:\n",
        "            print(\"No results to export. Please run analysis first.\")\n",
        "            return\n",
        "\n",
        "        # Create detailed results dataframe\n",
        "        detailed_results = []\n",
        "        for result in self.all_results:\n",
        "            row = {\n",
        "                'file_name': result['file_name'],\n",
        "                'accuracy': result['overall_accuracy'],\n",
        "                'total_samples': result['total_samples']\n",
        "            }\n",
        "\n",
        "            # Add metrics\n",
        "            for metric in ['precision', 'recall', 'FAR', 'FRR', 'F1']:\n",
        "                row[metric] = result['metrics'][metric]\n",
        "\n",
        "            detailed_results.append(row)\n",
        "\n",
        "        # Create summary dataframe\n",
        "        summary_data = []\n",
        "        metrics = ['mean', 'std', 'min', 'max']\n",
        "\n",
        "        for metric in metrics:\n",
        "            row = {'metric': metric, 'overall_accuracy': self.aggregated_results['overall_accuracy'][metric]}\n",
        "\n",
        "            # Add performance metrics\n",
        "            for measure in ['precision', 'recall', 'FAR', 'FRR', 'F1']:\n",
        "                row[measure] = self.aggregated_results['metrics'][measure][metric]\n",
        "\n",
        "            summary_data.append(row)\n",
        "\n",
        "        # Export to CSV\n",
        "        pd.DataFrame(detailed_results).to_csv(f\"{output_path}_detailed.csv\", index=False)\n",
        "        pd.DataFrame(summary_data).to_csv(f\"{output_path}_summary.csv\", index=False)\n",
        "\n",
        "# Example usage:\n",
        "analyzer = BinaryBenchmarkAnalyzer()\n",
        "analyzer.load_json_files('survey_responses')  # Directory containing JSON files\n",
        "analyzer.aggregate_results()\n",
        "analyzer.print_detailed_results()\n",
        "analyzer.export_results_to_csv('human_benchmark_results')"
      ],
      "metadata": {
        "id": "VND9dyvGTNcz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4b58f7a-7538-40df-a929-ad3d9e435200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Aggregated Binary Classification Results\n",
            "============================================================\n",
            "Number of benchmarks analyzed: 30\n",
            "Total samples across all benchmarks: 450\n",
            "\n",
            "Overall Accuracy:\n",
            "Mean: 58.67% ± 16.55%\n",
            "Range: [33.33%, 86.67%]\n",
            "\n",
            "Classification Metrics:\n",
            "------------------------------------------------------------\n",
            "PRECISION:\n",
            "  Mean: 39.74% ± 21.88%\n",
            "  Range: [0.00%, 80.00%]\n",
            "RECALL:\n",
            "  Mean: 53.10% ± 32.30%\n",
            "  Range: [0.00%, 100.00%]\n",
            "FAR:\n",
            "  Mean: 38.63% ± 18.80%\n",
            "  Range: [9.09%, 77.78%]\n",
            "FRR:\n",
            "  Mean: 46.90% ± 32.30%\n",
            "  Range: [0.00%, 100.00%]\n",
            "F1:\n",
            "  Mean: 43.79% ± 24.70%\n",
            "  Range: [0.00%, 83.33%]\n",
            "\n",
            "Average Confusion Matrix:\n",
            "------------------------------------------------------------\n",
            "Actual vs Predicted (legitimate, fake)\n",
            "['2.67', '2.33']\n",
            "['3.87', '6.13']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9sr7CWuGQEJH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
