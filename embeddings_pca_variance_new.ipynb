{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38ef80fb-c639-4807-9ca8-a2660ca72605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 10:31:22.512077: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-17 10:31:22.562420: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-17 10:31:22.562450: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-17 10:31:22.563996: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-17 10:31:22.573483: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "759b114d-2b35-4b7e-92f6-f8342257627f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intro</th>\n",
       "      <th>Full Name</th>\n",
       "      <th>Workplace</th>\n",
       "      <th>Location</th>\n",
       "      <th>Connections</th>\n",
       "      <th>Photo</th>\n",
       "      <th>Followers</th>\n",
       "      <th>About</th>\n",
       "      <th>Experiences</th>\n",
       "      <th>Number of Experiences</th>\n",
       "      <th>...</th>\n",
       "      <th>Number of Scores</th>\n",
       "      <th>Languages</th>\n",
       "      <th>Number of Languages</th>\n",
       "      <th>Organizations</th>\n",
       "      <th>Number of Organizations</th>\n",
       "      <th>Interests</th>\n",
       "      <th>Number of Interests</th>\n",
       "      <th>Activities</th>\n",
       "      <th>Number of Activities</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'Full Name': 'chenxia (polly) Pei', 'Workplac...</td>\n",
       "      <td>chenxia (polly) Pei</td>\n",
       "      <td>Jiangsu Junyao mainly offer services to cement...</td>\n",
       "      <td>Wuxi, Jiangsu, China</td>\n",
       "      <td>500</td>\n",
       "      <td>No</td>\n",
       "      <td>717</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'0': {'Role': 'Sales', 'Workplace': 'Jiangsu ...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{'0': {'Interest': 'Trina Solar', 'ID': '69648...</td>\n",
       "      <td>4</td>\n",
       "      <td>{'chenxia-pei-80177594': {'Full Name': 'chenxi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'Full Name': 'NEHA CHANDOK', 'Workplace': 'So...</td>\n",
       "      <td>NEHA CHANDOK</td>\n",
       "      <td>Software Analyst</td>\n",
       "      <td>Noida, Uttar Pradesh, India</td>\n",
       "      <td>500</td>\n",
       "      <td>No</td>\n",
       "      <td>1340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'0': {'Role': 'Software Analyst', 'Workplace'...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Language 0': 'Bangla', 'Language 1': 'Englis...</td>\n",
       "      <td>3</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{'0': {'Interest': 'Mohamed El-Erian', 'ID': '...</td>\n",
       "      <td>8</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'Full Name': 'Mounika Mungamuri', 'Workplace'...</td>\n",
       "      <td>Mounika Mungamuri</td>\n",
       "      <td>Senior Consultant at Infosys</td>\n",
       "      <td>Hyderabad, Telangana, India</td>\n",
       "      <td>7</td>\n",
       "      <td>Yes</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'0': {'Role': 'Senior Consultant', 'Workplace...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'Full Name': 'Katarina Djuric', 'Workplace': ...</td>\n",
       "      <td>Katarina Djuric</td>\n",
       "      <td>--</td>\n",
       "      <td>Belgrade, Serbia</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'0': {'Role': 'Instructor', 'Workplace': 'GE'...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{'0': {'Interest': 'GE', 'ID': '1015', 'Catego...</td>\n",
       "      <td>1</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'Full Name': 'Rachel Lally', 'Workplace': '--...</td>\n",
       "      <td>Rachel Lally</td>\n",
       "      <td>--</td>\n",
       "      <td>Dublin, County Dublin, Ireland</td>\n",
       "      <td>61</td>\n",
       "      <td>Yes</td>\n",
       "      <td>61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'0': {'Role': 'Bartender', 'Workplace': \"O'Su...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{'0': {'Interest': 'Richard Branson', 'ID': 'r...</td>\n",
       "      <td>3</td>\n",
       "      <td>{'garyltravis': {'Full Name': 'Gary Travis', '...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>{'Full Name': 'Emily Williams', 'Workplace': \"...</td>\n",
       "      <td>Emily Williams</td>\n",
       "      <td>Market Research Analyst at L'Oreal</td>\n",
       "      <td>New York City, New York</td>\n",
       "      <td>106</td>\n",
       "      <td>No</td>\n",
       "      <td>717</td>\n",
       "      <td>{}</td>\n",
       "      <td>Market Research Analyst Elizabeth Arden Jan 20...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>3</td>\n",
       "      <td>{}</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>{'Full Name': 'Sarah Johnson', 'Workplace': 'D...</td>\n",
       "      <td>Sarah Johnson</td>\n",
       "      <td>Director of Marketing Strategy at Acme Inc.</td>\n",
       "      <td>San Francisco, California</td>\n",
       "      <td>102</td>\n",
       "      <td>No</td>\n",
       "      <td>6</td>\n",
       "      <td>{}</td>\n",
       "      <td>Director of Marketing Acme Inc. January 2018-P...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>5</td>\n",
       "      <td>{}</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>{'Full Name': 'Sarah Johnson', 'Workplace': 'M...</td>\n",
       "      <td>Sarah Johnson</td>\n",
       "      <td>Manager at Acme Inc.</td>\n",
       "      <td>New York City, New York</td>\n",
       "      <td>435</td>\n",
       "      <td>No</td>\n",
       "      <td>10</td>\n",
       "      <td>A results-driven and experienced professional ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>6</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>{'Full Name': 'Emma Williams', 'Workplace': 'M...</td>\n",
       "      <td>Emma Williams</td>\n",
       "      <td>Manager and Director at ABC Inc.</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>280</td>\n",
       "      <td>No</td>\n",
       "      <td>34</td>\n",
       "      <td>With over 10 years of experience in operations...</td>\n",
       "      <td>Operations Manager ABC Inc. January 2015- Pres...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>English Spanish</td>\n",
       "      <td>2</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>4</td>\n",
       "      <td>{}</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>{'Full Name': 'Rachel Johnson', 'Workplace': '...</td>\n",
       "      <td>Rachel Johnson</td>\n",
       "      <td>Market Development Manager at ABC Inc.</td>\n",
       "      <td>Dhaka, Bangladesh</td>\n",
       "      <td>102</td>\n",
       "      <td>No</td>\n",
       "      <td>26</td>\n",
       "      <td>{}</td>\n",
       "      <td>Market Development Manager ABC Inc. January 20...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>2</td>\n",
       "      <td>{}</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3600 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Intro            Full Name  \\\n",
       "0     {'Full Name': 'chenxia (polly) Pei', 'Workplac...  chenxia (polly) Pei   \n",
       "1     {'Full Name': 'NEHA CHANDOK', 'Workplace': 'So...         NEHA CHANDOK   \n",
       "2     {'Full Name': 'Mounika Mungamuri', 'Workplace'...    Mounika Mungamuri   \n",
       "3     {'Full Name': 'Katarina Djuric', 'Workplace': ...      Katarina Djuric   \n",
       "4     {'Full Name': 'Rachel Lally', 'Workplace': '--...         Rachel Lally   \n",
       "...                                                 ...                  ...   \n",
       "3595  {'Full Name': 'Emily Williams', 'Workplace': \"...       Emily Williams   \n",
       "3596  {'Full Name': 'Sarah Johnson', 'Workplace': 'D...        Sarah Johnson   \n",
       "3597  {'Full Name': 'Sarah Johnson', 'Workplace': 'M...        Sarah Johnson   \n",
       "3598  {'Full Name': 'Emma Williams', 'Workplace': 'M...        Emma Williams   \n",
       "3599  {'Full Name': 'Rachel Johnson', 'Workplace': '...       Rachel Johnson   \n",
       "\n",
       "                                              Workplace  \\\n",
       "0     Jiangsu Junyao mainly offer services to cement...   \n",
       "1                                      Software Analyst   \n",
       "2                          Senior Consultant at Infosys   \n",
       "3                                                    --   \n",
       "4                                                    --   \n",
       "...                                                 ...   \n",
       "3595                 Market Research Analyst at L'Oreal   \n",
       "3596        Director of Marketing Strategy at Acme Inc.   \n",
       "3597                               Manager at Acme Inc.   \n",
       "3598                   Manager and Director at ABC Inc.   \n",
       "3599             Market Development Manager at ABC Inc.   \n",
       "\n",
       "                            Location  Connections Photo  Followers  \\\n",
       "0               Wuxi, Jiangsu, China          500    No        717   \n",
       "1        Noida, Uttar Pradesh, India          500    No       1340   \n",
       "2        Hyderabad, Telangana, India            7   Yes          7   \n",
       "3                   Belgrade, Serbia            0   Yes          0   \n",
       "4     Dublin, County Dublin, Ireland           61   Yes         61   \n",
       "...                              ...          ...   ...        ...   \n",
       "3595         New York City, New York          106    No        717   \n",
       "3596       San Francisco, California          102    No          6   \n",
       "3597         New York City, New York          435    No         10   \n",
       "3598             Seattle, Washington          280    No         34   \n",
       "3599               Dhaka, Bangladesh          102    No         26   \n",
       "\n",
       "                                                  About  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "3595                                                 {}   \n",
       "3596                                                 {}   \n",
       "3597  A results-driven and experienced professional ...   \n",
       "3598  With over 10 years of experience in operations...   \n",
       "3599                                                 {}   \n",
       "\n",
       "                                            Experiences  \\\n",
       "0     {'0': {'Role': 'Sales', 'Workplace': 'Jiangsu ...   \n",
       "1     {'0': {'Role': 'Software Analyst', 'Workplace'...   \n",
       "2     {'0': {'Role': 'Senior Consultant', 'Workplace...   \n",
       "3     {'0': {'Role': 'Instructor', 'Workplace': 'GE'...   \n",
       "4     {'0': {'Role': 'Bartender', 'Workplace': \"O'Su...   \n",
       "...                                                 ...   \n",
       "3595  Market Research Analyst Elizabeth Arden Jan 20...   \n",
       "3596  Director of Marketing Acme Inc. January 2018-P...   \n",
       "3597                                                 {}   \n",
       "3598  Operations Manager ABC Inc. January 2015- Pres...   \n",
       "3599  Market Development Manager ABC Inc. January 20...   \n",
       "\n",
       "      Number of Experiences  ... Number of Scores  \\\n",
       "0                         2  ...                0   \n",
       "1                         1  ...                0   \n",
       "2                         2  ...                0   \n",
       "3                         1  ...                0   \n",
       "4                         1  ...                1   \n",
       "...                     ...  ...              ...   \n",
       "3595                      3  ...                0   \n",
       "3596                      5  ...                0   \n",
       "3597                      0  ...                0   \n",
       "3598                      1  ...                0   \n",
       "3599                      5  ...                0   \n",
       "\n",
       "                                              Languages Number of Languages  \\\n",
       "0                                                    {}                   0   \n",
       "1     {'Language 0': 'Bangla', 'Language 1': 'Englis...                   3   \n",
       "2                                                    {}                   0   \n",
       "3                                                    {}                   0   \n",
       "4                                                    {}                   0   \n",
       "...                                                 ...                 ...   \n",
       "3595                                                 {}                   0   \n",
       "3596                                                 {}                   0   \n",
       "3597                                                 {}                   0   \n",
       "3598                                    English Spanish                   2   \n",
       "3599                                                 {}                   0   \n",
       "\n",
       "      Organizations Number of Organizations  \\\n",
       "0                {}                       0   \n",
       "1                {}                       0   \n",
       "2                {}                       0   \n",
       "3                {}                       0   \n",
       "4                {}                       0   \n",
       "...             ...                     ...   \n",
       "3595             {}                       0   \n",
       "3596             {}                       0   \n",
       "3597             {}                       0   \n",
       "3598             {}                       0   \n",
       "3599             {}                       0   \n",
       "\n",
       "                                              Interests Number of Interests  \\\n",
       "0     {'0': {'Interest': 'Trina Solar', 'ID': '69648...                   4   \n",
       "1     {'0': {'Interest': 'Mohamed El-Erian', 'ID': '...                   8   \n",
       "2                                                    {}                   0   \n",
       "3     {'0': {'Interest': 'GE', 'ID': '1015', 'Catego...                   1   \n",
       "4     {'0': {'Interest': 'Richard Branson', 'ID': 'r...                   3   \n",
       "...                                                 ...                 ...   \n",
       "3595                                                 {}                   3   \n",
       "3596                                                 {}                   5   \n",
       "3597                                                 {}                   6   \n",
       "3598                                                 {}                   4   \n",
       "3599                                                 {}                   2   \n",
       "\n",
       "                                             Activities Number of Activities  \\\n",
       "0     {'chenxia-pei-80177594': {'Full Name': 'chenxi...                    1   \n",
       "1                                                    {}                    0   \n",
       "2                                                    {}                    0   \n",
       "3                                                    {}                    0   \n",
       "4     {'garyltravis': {'Full Name': 'Gary Travis', '...                    6   \n",
       "...                                                 ...                  ...   \n",
       "3595                                                 {}                    4   \n",
       "3596                                                 {}                    3   \n",
       "3597                                                 {}                    0   \n",
       "3598                                                 {}                    4   \n",
       "3599                                                 {}                   10   \n",
       "\n",
       "      Label  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  \n",
       "...     ...  \n",
       "3595     11  \n",
       "3596     11  \n",
       "3597     11  \n",
       "3598     11  \n",
       "3599     11  \n",
       "\n",
       "[3600 rows x 39 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read from CSV instead of pickle\n",
    "dataset = pd.read_csv('cleaned_profiles.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c5a9173-c6a6-487c-848e-e9e72edf3f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/f20210934/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Found existing combined results with completed models: bert, modernbert, deberta, gemini, flair, glove, roberta\n",
      "\n",
      "\n",
      "===== Skipping flair (already fully processed) =====\n",
      "\n",
      "\n",
      "===== Skipping glove (already fully processed) =====\n",
      "\n",
      "\n",
      "===== Skipping roberta (already fully processed) =====\n",
      "\n",
      "\n",
      "===== Skipping bert (already fully processed) =====\n",
      "\n",
      "\n",
      "===== Skipping modernbert (already fully processed) =====\n",
      "\n",
      "\n",
      "===== Skipping deberta (already fully processed) =====\n",
      "All processing complete. Files saved with suffix: \n",
      "Output directories: embeddings_output and variance_plots\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "import ast\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AutoTokenizer, AutoModel, RobertaTokenizer, RobertaModel\n",
    "import gensim.downloader as gensim_downloader\n",
    "from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings\n",
    "from flair.data import Sentence\n",
    "\n",
    "# Simple generated suffix for file names\n",
    "FILE_SUFFIX = \"\"\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Create output directories with suffix\n",
    "output_dir = f'embeddings_output'\n",
    "plots_dir = f'variance_plots'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "# Define numerical fields (used in multiple places)\n",
    "NUMERICAL_FIELDS = [\n",
    "    \"Number of Experiences\", \"Number of Educations\", \"Number of Licenses\",\n",
    "    \"Number of Volunteering\", \"Number of Skills\", \"Number of Recommendations\",\n",
    "    \"Number of Projects\", \"Number of Publications\", \"Number of Courses\",\n",
    "    \"Number of Honors\", \"Number of Scores\", \"Number of Languages\",\n",
    "    \"Number of Organizations\", \"Number of Interests\", \"Number of Activities\",\n",
    "    \"Connections\", \"Followers\"\n",
    "]\n",
    "\n",
    "# Column names for numerical features (used in output dataframes)\n",
    "NUMERICAL_FIELD_NAMES = [\n",
    "    \"num_experiences\", \"num_educations\", \"num_licenses\",\n",
    "    \"num_volunteering\", \"num_skills\", \"num_recommendations\",\n",
    "    \"num_projects\", \"num_publications\", \"num_courses\",\n",
    "    \"num_honors\", \"num_scores\", \"num_languages\",\n",
    "    \"num_organizations\", \"num_interests\", \"num_activities\",\n",
    "    \"connections\", \"followers\"\n",
    "]\n",
    "\n",
    "# Model configurations\n",
    "model_configs = [\n",
    "    {\n",
    "        \"name\": \"flair\",\n",
    "        \"model_name\": \"en\",  # Flair embeddings for English (300d)\n",
    "        \"output_file\": os.path.join(output_dir, f'flair_PCA_scaled_embeddings{FILE_SUFFIX}.csv'),\n",
    "        \"temp_file\": os.path.join(output_dir, f'flair_temp_embeddings{FILE_SUFFIX}.pkl'),\n",
    "        \"variance_plot\": os.path.join(plots_dir, f'flair_explained_variance{FILE_SUFFIX}.png')\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"glove\",\n",
    "        \"model_name\": \"glove.6B.300d.txt\",  # GloVe embeddings (300d)\n",
    "        \"output_file\": os.path.join(output_dir, f'glove_PCA_scaled_embeddings{FILE_SUFFIX}.csv'),\n",
    "        \"temp_file\": os.path.join(output_dir, f'glove_temp_embeddings{FILE_SUFFIX}.pkl'),\n",
    "        \"variance_plot\": os.path.join(plots_dir, f'glove_explained_variance{FILE_SUFFIX}.png')\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"roberta\",\n",
    "        \"model_name\": \"roberta-base\",\n",
    "        \"output_file\": os.path.join(output_dir, f'roberta_PCA_scaled_embeddings{FILE_SUFFIX}.csv'),\n",
    "        \"temp_file\": os.path.join(output_dir, f'roberta_temp_embeddings{FILE_SUFFIX}.pkl'),\n",
    "        \"variance_plot\": os.path.join(plots_dir, f'roberta_explained_variance{FILE_SUFFIX}.png')\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"bert\",\n",
    "        \"model_name\": \"bert-base-uncased\",\n",
    "        \"output_file\": os.path.join(output_dir, f'bert_PCA_scaled_embeddings{FILE_SUFFIX}.csv'),\n",
    "        \"temp_file\": os.path.join(output_dir, f'bert_temp_embeddings{FILE_SUFFIX}.pkl'),\n",
    "        \"variance_plot\": os.path.join(plots_dir, f'bert_explained_variance{FILE_SUFFIX}.png')\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"modernbert\",\n",
    "        \"model_name\": \"answerdotai/ModernBERT-base\",\n",
    "        \"output_file\": os.path.join(output_dir, f'modernbert_PCA_scaled_embeddings{FILE_SUFFIX}.csv'),\n",
    "        \"temp_file\": os.path.join(output_dir, f'modernbert_temp_embeddings{FILE_SUFFIX}.pkl'),\n",
    "        \"variance_plot\": os.path.join(plots_dir, f'modernbert_explained_variance{FILE_SUFFIX}.png')\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"deberta\",\n",
    "        \"model_name\": \"microsoft/deberta-v3-large\",\n",
    "        \"output_file\": os.path.join(output_dir, f'deberta_PCA_scaled_embeddings{FILE_SUFFIX}.csv'),\n",
    "        \"temp_file\": os.path.join(output_dir, f'deberta_temp_embeddings{FILE_SUFFIX}.pkl'),\n",
    "        \"variance_plot\": os.path.join(plots_dir, f'deberta_explained_variance{FILE_SUFFIX}.png')\n",
    "    }\n",
    "]\n",
    "\n",
    "# PCA components to try\n",
    "PCA_COMPONENTS_OPTIONS = [10, 20, 30, 50, 75, 100, 125, 150, 175, 200, 250, 300, 350, 400]\n",
    "\n",
    "# Target explained variance threshold\n",
    "TARGET_VARIANCE = 0.95\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Global variables for models\n",
    "glove_model = None\n",
    "flair_embedding = None\n",
    "\n",
    "\n",
    "def safe_eval(s):\n",
    "    \"\"\"Safely evaluate a string to convert it to a Python object\"\"\"\n",
    "    if isinstance(s, str):\n",
    "        try:\n",
    "            return ast.literal_eval(s)\n",
    "        except:\n",
    "            return s\n",
    "    return s\n",
    "\n",
    "\n",
    "def get_embedding_transformer(text, tokenizer, model):\n",
    "    \"\"\"Get embeddings for transformer-based models (BERT, RoBERTa, etc.)\"\"\"\n",
    "    if not text or text.strip() == \"\":\n",
    "        # Handle empty text by returning zeros\n",
    "        with torch.no_grad():\n",
    "            dummy_inputs = tokenizer(\"test\", return_tensors='pt').to(device)\n",
    "            dummy_outputs = model(**dummy_inputs)\n",
    "            if hasattr(dummy_outputs, 'last_hidden_state'):\n",
    "                emb_dim = dummy_outputs.last_hidden_state.shape[-1]\n",
    "            elif hasattr(dummy_outputs, 'pooler_output'):\n",
    "                emb_dim = dummy_outputs.pooler_output.shape[-1]\n",
    "            else:\n",
    "                emb_dim = 768  # default fallback\n",
    "        return np.zeros(emb_dim)\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Different models have different output structures\n",
    "    if hasattr(outputs, 'last_hidden_state'):\n",
    "        # Standard transformer models like BERT, RoBERTa, DeBERTa\n",
    "        return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "    elif hasattr(outputs, 'pooler_output'):\n",
    "        # Some models provide pooled output\n",
    "        return outputs.pooler_output.squeeze().cpu().numpy()\n",
    "    else:\n",
    "        # Fallback - use whatever tensor is available\n",
    "        for key, value in outputs.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                return value.mean(dim=1).squeeze().cpu().numpy()\n",
    "    \n",
    "    raise ValueError(f\"Could not extract embeddings from model outputs: {outputs}\")\n",
    "\n",
    "\n",
    "def get_embedding_glove(text):\n",
    "    \"\"\"Get embeddings using GloVe model\"\"\"\n",
    "    global glove_model\n",
    "    \n",
    "    if not text or text.strip() == \"\":\n",
    "        return np.zeros(300)  # GloVe 6B.300d has 300 dimensions\n",
    "    \n",
    "    # Tokenize text into words\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Get embeddings for each word\n",
    "    word_embeddings = []\n",
    "    for word in words:\n",
    "        if word in glove_model:\n",
    "            word_embeddings.append(glove_model[word])\n",
    "    \n",
    "    # Return average of word embeddings\n",
    "    if word_embeddings:\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(300)\n",
    "\n",
    "\n",
    "def get_embedding_flair(text):\n",
    "    \"\"\"Get embeddings using Flair\"\"\"\n",
    "    global flair_embedding\n",
    "    \n",
    "    if not text or text.strip() == \"\":\n",
    "        return np.zeros(300)  # Flair \"en\" has 300 dimensions\n",
    "    \n",
    "    # Create a Flair Sentence\n",
    "    sentence = Sentence(text)\n",
    "    \n",
    "    # Embed the sentence\n",
    "    flair_embedding.embed(sentence)\n",
    "    \n",
    "    # Get the embedding vector\n",
    "    embedding_vector = sentence.embedding.cpu().numpy()\n",
    "    \n",
    "    return embedding_vector\n",
    "\n",
    "\n",
    "def apply_ste_transformer(content_embedding, section_tag, tokenizer, model):\n",
    "    tag_embedding = get_embedding_transformer(section_tag, tokenizer, model)\n",
    "    return content_embedding - tag_embedding\n",
    "\n",
    "\n",
    "def apply_ste_glove(content_embedding, section_tag):\n",
    "    tag_embedding = get_embedding_glove(section_tag)\n",
    "    return content_embedding - tag_embedding\n",
    "\n",
    "\n",
    "def apply_ste_flair(content_embedding, section_tag):\n",
    "    tag_embedding = get_embedding_flair(section_tag)\n",
    "    return content_embedding - tag_embedding\n",
    "\n",
    "\n",
    "def process_row(index, dataset, model_name, tokenizer=None, model=None):\n",
    "    \"\"\"Process a single row of data, extracting text and generating embeddings\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Extract data\n",
    "    intro = safe_eval(dataset[\"Intro\"].iloc[index])\n",
    "    about = safe_eval(dataset[\"About\"].iloc[index])\n",
    "    experiences = safe_eval(dataset[\"Experiences\"].iloc[index])\n",
    "\n",
    "    # Process intro data\n",
    "    if isinstance(intro, dict):\n",
    "        full_name = intro.get(\"Full Name\", \"\")\n",
    "        workplace = intro.get(\"Workplace\", \"\")\n",
    "        location = intro.get(\"Location\", \"\")\n",
    "    else:\n",
    "        full_name = workplace = location = str(intro)\n",
    "\n",
    "    # Process about data\n",
    "    if isinstance(about, dict):\n",
    "        desc = about.get(\"About\", \"\")\n",
    "    else:\n",
    "        desc = str(about)\n",
    "\n",
    "    # Process experiences data\n",
    "    exp_text = \"\"\n",
    "    if isinstance(experiences, list):\n",
    "        for exp in experiences:\n",
    "            if isinstance(exp, dict):\n",
    "                exp_text += f\"{exp.get('Role', '')} {exp.get('Workplace', '')} {exp.get('Duration', '')} {exp.get('Workplace Location', '')} {exp.get('Description', '')} \"\n",
    "    elif isinstance(experiences, dict):\n",
    "        for value in experiences.values():\n",
    "            if isinstance(value, dict):\n",
    "                exp_text += f\"{value.get('Role', '')} {value.get('Workplace', '')} {value.get('Duration', '')} {value.get('Workplace Location', '')} {value.get('Description', '')} \"\n",
    "\n",
    "    # Extract numerical fields\n",
    "    numerical_data = []\n",
    "    for field in NUMERICAL_FIELDS:\n",
    "        value = dataset[field].iloc[index]\n",
    "        if pd.isna(value):\n",
    "            numerical_data.append(0)\n",
    "        elif isinstance(value, (int, float)):\n",
    "            numerical_data.append(value)\n",
    "        else:\n",
    "            try:\n",
    "                numerical_data.append(float(value))\n",
    "            except:\n",
    "                numerical_data.append(0)  # Default to 0 if conversion fails\n",
    "\n",
    "    # STE Implementation based on model type\n",
    "    if model_name == \"glove\":\n",
    "        # Intro section\n",
    "        intro_emb = get_embedding_glove(f\"{full_name} {workplace} {location}\")\n",
    "        fin1 = apply_ste_glove(intro_emb, \"Introduction\")\n",
    "\n",
    "        # About section\n",
    "        about_emb = get_embedding_glove(desc)\n",
    "        fin2 = apply_ste_glove(about_emb, \"About\")\n",
    "\n",
    "        # Experiences section\n",
    "        exp_emb = get_embedding_glove(exp_text)\n",
    "        fin3 = apply_ste_glove(exp_emb, \"Experiences\")\n",
    "    elif model_name == \"flair\":\n",
    "        # Intro section\n",
    "        intro_emb = get_embedding_flair(f\"{full_name} {workplace} {location}\")\n",
    "        fin1 = apply_ste_flair(intro_emb, \"Introduction\")\n",
    "\n",
    "        # About section\n",
    "        about_emb = get_embedding_flair(desc)\n",
    "        fin2 = apply_ste_flair(about_emb, \"About\")\n",
    "\n",
    "        # Experiences section\n",
    "        exp_emb = get_embedding_flair(exp_text)\n",
    "        fin3 = apply_ste_flair(exp_emb, \"Experiences\")\n",
    "    else:  # Transformer models (bert, modernbert, deberta, roberta)\n",
    "        # Intro section\n",
    "        intro_emb = get_embedding_transformer(f\"{full_name} {workplace} {location}\", tokenizer, model)\n",
    "        fin1 = apply_ste_transformer(intro_emb, \"Introduction\", tokenizer, model)\n",
    "\n",
    "        # About section\n",
    "        about_emb = get_embedding_transformer(desc, tokenizer, model)\n",
    "        fin2 = apply_ste_transformer(about_emb, \"About\", tokenizer, model)\n",
    "\n",
    "        # Experiences section\n",
    "        exp_emb = get_embedding_transformer(exp_text, tokenizer, model)\n",
    "        fin3 = apply_ste_transformer(exp_emb, \"Experiences\", tokenizer, model)\n",
    "\n",
    "    # Average the text embeddings\n",
    "    text_embedding = (fin1 + fin2 + fin3) / 3\n",
    "\n",
    "    # Store text embedding and numerical data separately\n",
    "    result = {\n",
    "        'text_embedding': text_embedding,\n",
    "        'numerical': np.array(numerical_data)\n",
    "    }\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    logging.info(f\"Processing time for row {index}: {execution_time:.2f} seconds\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_embedding_dimensions(model_name):\n",
    "    \"\"\"Return default embedding dimensions for different models\"\"\"\n",
    "    if model_name == \"glove\":\n",
    "        return 300  # Using 300d as specified\n",
    "    elif model_name == \"flair\":\n",
    "        return 300  # Using 300d as specified\n",
    "    elif model_name == \"deberta\":\n",
    "        return 1024  # DeBERTa large has 1024 dimensions\n",
    "    else:  # bert, roberta, modernbert\n",
    "        return 768\n",
    "\n",
    "\n",
    "def generate_embeddings(dataset, model_config, num_rows=3600, save_interval=100):\n",
    "    \"\"\"Generate embeddings for the dataset using the specified model\"\"\"\n",
    "    model_name = model_config[\"name\"]\n",
    "    temp_file = model_config[\"temp_file\"]\n",
    "    model_path = model_config[\"model_name\"]\n",
    "    \n",
    "    print(f\"Loading {model_name} model: {model_path}\")\n",
    "    \n",
    "    tokenizer = None\n",
    "    model = None\n",
    "    \n",
    "    # Initialize models based on model type\n",
    "    global glove_model, flair_embedding\n",
    "    \n",
    "    if model_name == \"glove\":\n",
    "        # Load GloVe model\n",
    "        try:\n",
    "            # Try to load from file first\n",
    "            glove_model = {}\n",
    "            with open(model_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    values = line.strip().split()\n",
    "                    word = values[0]\n",
    "                    vector = np.array([float(val) for val in values[1:]])\n",
    "                    glove_model[word] = vector\n",
    "            print(f\"Loaded GloVe from file: {model_path}\")\n",
    "        except FileNotFoundError:\n",
    "            # If file not found, download using gensim\n",
    "            print(f\"File not found, downloading GloVe...\")\n",
    "            glove_model = gensim_downloader.load(\"glove-wiki-gigaword-300\")\n",
    "            print(\"Downloaded GloVe 300d model\")\n",
    "    \n",
    "    elif model_name == \"flair\":\n",
    "        # Load Flair embeddings\n",
    "        word_embeddings = WordEmbeddings('en')\n",
    "        flair_embedding = DocumentPoolEmbeddings([word_embeddings])\n",
    "        print(\"Loaded Flair embeddings\")\n",
    "    \n",
    "    elif model_name == \"roberta\":\n",
    "        # Load RoBERTa model\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "        model = RobertaModel.from_pretrained(model_path)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        print(\"Loaded RoBERTa model\")\n",
    "    \n",
    "    elif model_name == \"modernbert\":\n",
    "        # Load ModernBERT model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModel.from_pretrained(model_path)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        print(\"Loaded ModernBERT model\")\n",
    "    \n",
    "    else:  # bert or deberta\n",
    "        # Load BERT/DeBERTa model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModel.from_pretrained(model_path)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        print(f\"Loaded {model_name} model\")\n",
    "    \n",
    "    all_embeddings = []\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    # Check if there's a partially processed file\n",
    "    try:\n",
    "        existing_df = pd.read_pickle(temp_file)\n",
    "        start_index = len(existing_df)\n",
    "        all_embeddings = existing_df\n",
    "        print(f\"Resuming from row {start_index} for {model_name}\")\n",
    "    except (FileNotFoundError, EOFError):\n",
    "        start_index = 0\n",
    "        print(f\"Starting new embedding generation for {model_name}\")\n",
    "\n",
    "    for index in range(start_index, min(num_rows, len(dataset))):\n",
    "        row_start_time = time.time()\n",
    "        try:\n",
    "            embeddings_dict = process_row(index, dataset, model_name, tokenizer, model)\n",
    "            all_embeddings.append(embeddings_dict)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {index} with {model_name}: {str(e)}\")\n",
    "            \n",
    "            # Create a dummy embeddings dict with zeros\n",
    "            emb_dim = get_embedding_dimensions(model_name)\n",
    "            \n",
    "            dummy_dict = {\n",
    "                'text_embedding': np.zeros(emb_dim),\n",
    "                'numerical': np.zeros(len(NUMERICAL_FIELDS))\n",
    "            }\n",
    "            all_embeddings.append(dummy_dict)\n",
    "\n",
    "        row_end_time = time.time()\n",
    "        row_execution_time = row_end_time - row_start_time\n",
    "        print(f\"{model_name}: Processed row {index + 1} in {row_execution_time:.2f} seconds\")\n",
    "\n",
    "        # Save progress at specified intervals\n",
    "        if (index + 1) % save_interval == 0:\n",
    "            pd.to_pickle(all_embeddings, temp_file)\n",
    "            print(f\"{model_name}: Progress saved at row {index + 1}\")\n",
    "\n",
    "    # Final save of the raw embeddings\n",
    "    pd.to_pickle(all_embeddings, temp_file)\n",
    "    print(f\"{model_name}: All raw embeddings saved to {temp_file}\")\n",
    "    \n",
    "    # Free up memory for transformer models\n",
    "    if model_name in [\"bert\", \"modernbert\", \"deberta\", \"roberta\"]:\n",
    "        del model\n",
    "        del tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return all_embeddings\n",
    "\n",
    "\n",
    "def analyze_pca_variance(embeddings_list, model_config):\n",
    "    \"\"\"Analyze PCA explained variance for different numbers of components\"\"\"\n",
    "    # Check if variance plot already exists\n",
    "    if os.path.exists(model_config[\"variance_plot\"]):\n",
    "        print(f\"Variance plot already exists for {model_config['name']}, skipping plot generation\")\n",
    "    \n",
    "    # Extract text embeddings\n",
    "    text_embeddings = np.vstack([emb['text_embedding'].reshape(1, -1) for emb in embeddings_list])\n",
    "    embedding_dim = text_embeddings.shape[1]\n",
    "    \n",
    "    # Determine maximum components based on model type\n",
    "    model_name = model_config['name']\n",
    "    if model_name == \"glove\":\n",
    "        max_components = min(embedding_dim, 300)  # GloVe 300d dimensions\n",
    "    elif model_name == \"flair\":\n",
    "        max_components = min(embedding_dim, 300)  # Flair \"en\" has 300 dimensions\n",
    "    else:\n",
    "        # For transformer models, use the standard cap\n",
    "        max_components = min(text_embeddings.shape[0], text_embeddings.shape[1])\n",
    "        max_components = min(max_components, 400)\n",
    "    \n",
    "    print(f\"{model_name} embedding dimension: {embedding_dim}, using max {max_components} components for PCA\")\n",
    "    \n",
    "    # Initialize PCA with maximum number of components\n",
    "    pca = PCA(n_components=max_components)\n",
    "    pca.fit(text_embeddings)\n",
    "    \n",
    "    # Calculate cumulative explained variance\n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    # Find number of components needed for target variance\n",
    "    if np.max(cumulative_variance) < TARGET_VARIANCE:\n",
    "        print(f\"Warning: {model_config['name']} cannot reach {TARGET_VARIANCE*100}% variance with {max_components} components\")\n",
    "        print(f\"Maximum achievable variance is {np.max(cumulative_variance)*100:.2f}%\")\n",
    "        components_for_target = max_components\n",
    "    else:\n",
    "        components_for_target = np.argmax(cumulative_variance >= TARGET_VARIANCE) + 1\n",
    "    \n",
    "    print(f\"{model_config['name']}: {components_for_target} components needed for {TARGET_VARIANCE*100}% variance\")\n",
    "    \n",
    "    # Create the plot if it doesn't exist\n",
    "    if not os.path.exists(model_config[\"variance_plot\"]):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='-', markersize=3)\n",
    "        plt.axhline(y=TARGET_VARIANCE, color='r', linestyle='--', label=f\"{TARGET_VARIANCE*100}% Explained Variance\")\n",
    "        plt.axvline(x=components_for_target, color='g', linestyle='--', \n",
    "                    label=f\"{components_for_target} components\")\n",
    "        \n",
    "        # Add vertical lines for each PCA component option - only for valid components\n",
    "        valid_component_options = [c for c in PCA_COMPONENTS_OPTIONS if c <= max_components]\n",
    "        for comp in valid_component_options:\n",
    "            plt.axvline(x=comp, color='gray', alpha=0.3, linestyle=':')\n",
    "        \n",
    "        plt.title(f'Cumulative Explained Variance: {model_config[\"name\"].upper()}')\n",
    "        plt.xlabel('Number of Components')\n",
    "        plt.ylabel('Cumulative Explained Variance')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(model_config[\"variance_plot\"])\n",
    "        plt.close()\n",
    "    \n",
    "    # Create a dataframe with detailed variance information for all components\n",
    "    variance_df = pd.DataFrame({\n",
    "        'component': range(1, len(pca.explained_variance_ratio_) + 1),\n",
    "        'explained_variance': pca.explained_variance_ratio_,\n",
    "        'cumulative_variance': cumulative_variance\n",
    "    })\n",
    "    \n",
    "    # Save the variance data\n",
    "    variance_file = os.path.join(output_dir, f\"{model_config['name']}_variance_data{FILE_SUFFIX}.csv\")\n",
    "    \n",
    "    # Check if we need to save the variance data\n",
    "    if not os.path.exists(variance_file):\n",
    "        variance_df.to_csv(variance_file, index=False)\n",
    "        print(f\"Variance data saved to {variance_file}\")\n",
    "    else:\n",
    "        print(f\"Variance data already exists at {variance_file}\")\n",
    "    \n",
    "    # Return variance data for further analysis\n",
    "    variance_data = {\n",
    "        'n_components': list(range(1, len(cumulative_variance) + 1)),\n",
    "        'explained_variance': pca.explained_variance_ratio_,\n",
    "        'cumulative_variance': cumulative_variance,\n",
    "        'optimal_components': components_for_target\n",
    "    }\n",
    "    \n",
    "    return variance_data, pca\n",
    "\n",
    "\n",
    "def analyze_pca_variance_with_checkpoint(embeddings_list, model_config):\n",
    "    \"\"\"Analyze PCA explained variance with checkpoint support\"\"\"\n",
    "    variance_file = os.path.join(output_dir, f\"{model_config['name']}_variance_data{FILE_SUFFIX}.csv\")\n",
    "    \n",
    "    # Check if the variance data file already exists\n",
    "    if os.path.exists(variance_file):\n",
    "        try:\n",
    "            variance_df = pd.read_csv(variance_file)\n",
    "            print(f\"Found existing variance data for {model_config['name']}, skipping analysis\")\n",
    "            \n",
    "            # Extract optimal components from existing data\n",
    "            cumulative_variance = variance_df['cumulative_variance'].values\n",
    "            components_for_target = np.argmax(cumulative_variance >= TARGET_VARIANCE) + 1\n",
    "            \n",
    "            variance_data = {\n",
    "                'n_components': variance_df['component'].tolist(),\n",
    "                'explained_variance': variance_df['explained_variance'].values,\n",
    "                'cumulative_variance': cumulative_variance,\n",
    "                'optimal_components': components_for_target\n",
    "            }\n",
    "            \n",
    "            print(f\"{model_config['name']}: {components_for_target} components needed for {TARGET_VARIANCE*100}% variance\")\n",
    "            \n",
    "            return variance_data, None\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading existing variance data: {str(e)}\")\n",
    "            print(\"Will recompute variance analysis\")\n",
    "    \n",
    "    # If no valid existing file, perform the analysis\n",
    "    return analyze_pca_variance(embeddings_list, model_config)\n",
    "\n",
    "\n",
    "def generate_pca_embeddings(embeddings_list, n_components):\n",
    "    \"\"\"Generate PCA-reduced embeddings with standardized numerical features\"\"\"\n",
    "    # Extract text and numerical embeddings\n",
    "    text_embeddings = np.vstack([emb['text_embedding'].reshape(1, -1) for emb in embeddings_list])\n",
    "    numerical_features = np.vstack([emb['numerical'].reshape(1, -1) for emb in embeddings_list])\n",
    "    \n",
    "    # Apply PCA to reduce dimensions of text embeddings\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_text_embeddings = pca.fit_transform(text_embeddings)\n",
    "    \n",
    "    # Print explained variance ratio\n",
    "    explained_variance = np.sum(pca.explained_variance_ratio_)\n",
    "    print(f\"PCA with {n_components} components explains {explained_variance:.4f} of variance\")\n",
    "    \n",
    "    # Scale the numerical features\n",
    "    print(\"Scaling numerical features...\")\n",
    "    scaler = StandardScaler()\n",
    "    scaled_numerical_features = scaler.fit_transform(numerical_features)\n",
    "    \n",
    "    # Concatenate reduced text embeddings with scaled numerical features\n",
    "    final_embeddings = np.hstack([reduced_text_embeddings, scaled_numerical_features])\n",
    "    \n",
    "    # Create column names for the DataFrame\n",
    "    columns = [f\"pca_{i}\" for i in range(n_components)]\n",
    "    columns.extend(NUMERICAL_FIELD_NAMES)\n",
    "    \n",
    "    return final_embeddings, columns, pca.explained_variance_ratio_.sum()\n",
    "\n",
    "\n",
    "def generate_embeddings_for_all_components(embeddings_list, model_config):\n",
    "    \"\"\"Generate embeddings for all PCA component options\"\"\"\n",
    "    results = []\n",
    "    summary_file = os.path.join(output_dir, f\"{model_config['name']}_embeddings_summary{FILE_SUFFIX}.csv\")\n",
    "    \n",
    "    # Check model dimensionality limits\n",
    "    model_name = model_config['name']\n",
    "    text_embeddings = np.vstack([emb['text_embedding'].reshape(1, -1) for emb in embeddings_list])\n",
    "    embedding_dim = text_embeddings.shape[1]\n",
    "    \n",
    "    # Define max components based on model type - only limit for GloVe and Flair\n",
    "    if model_name == \"glove\":\n",
    "        max_allowed_components = min(embedding_dim, 300)  # GloVe 300d dimensions\n",
    "        print(f\"GloVe has {embedding_dim} dimensions, limiting components to {max_allowed_components}\")\n",
    "    elif model_name == \"flair\":\n",
    "        max_allowed_components = min(embedding_dim, 300)  # Flair 300d dimensions\n",
    "        print(f\"Flair has {embedding_dim} dimensions, limiting components to {max_allowed_components}\")\n",
    "    else:\n",
    "        max_allowed_components = 400  # No specific limit for transformer models\n",
    "    \n",
    "    # Filter component options based on model's dimensionality\n",
    "    valid_component_options = [c for c in PCA_COMPONENTS_OPTIONS if c <= max_allowed_components]\n",
    "    \n",
    "    if len(valid_component_options) < len(PCA_COMPONENTS_OPTIONS):\n",
    "        print(f\"Note: Only using components up to {max_allowed_components} for {model_name} due to dimensionality constraints\")\n",
    "    \n",
    "    # Check if summary file exists - if so, we can resume from where we left off\n",
    "    try:\n",
    "        existing_results = pd.read_csv(summary_file)\n",
    "        # Get the components that have already been processed\n",
    "        completed_components = existing_results['n_components'].tolist()\n",
    "        results = existing_results.to_dict('records')\n",
    "        print(f\"Found existing summary for {model_config['name']} with {len(completed_components)} completed component sizes\")\n",
    "    except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "        completed_components = []\n",
    "        print(f\"No existing summary found for {model_config['name']}, starting fresh\")\n",
    "    \n",
    "    for n_components in valid_component_options:\n",
    "        # Skip if this component size has already been processed\n",
    "        output_file = os.path.join(output_dir, f\"{model_config['name']}_PCA_{n_components}_components{FILE_SUFFIX}.csv\")\n",
    "        \n",
    "        if n_components in completed_components and os.path.exists(output_file):\n",
    "            print(f\"{model_config['name']}: Skipping {n_components} components (already processed)\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"{model_config['name']}: Generating embeddings with {n_components} components\")\n",
    "        \n",
    "        # Generate embeddings with this number of components\n",
    "        embeddings, column_names, variance = generate_pca_embeddings(embeddings_list, n_components)\n",
    "        \n",
    "        # Create DataFrame with embeddings\n",
    "        embeddings_df = pd.DataFrame(embeddings, columns=column_names)\n",
    "        embeddings_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(f\"Saved embeddings with {n_components} components to {output_file}\")\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'model': model_config['name'],\n",
    "            'n_components': n_components,\n",
    "            'explained_variance': variance,\n",
    "            'file_path': output_file\n",
    "        })\n",
    "        \n",
    "        # Update the summary file after each component size to facilitate resuming\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(summary_file, index=False)\n",
    "    \n",
    "    # Final save of the summary DataFrame (should be complete now)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(summary_file, index=False)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the embedding generation and analysis pipeline\"\"\"\n",
    "    # Load your dataset here\n",
    "    try:\n",
    "        # If you need to load the dataset from a file, uncomment and modify the following line\n",
    "        # dataset = pd.read_csv('your_dataset.csv')\n",
    "        \n",
    "        # Create results dataframe to collect all models' results\n",
    "        all_model_results = []\n",
    "        \n",
    "        # Track max components for each model for the comparative plot\n",
    "        model_max_components = {}\n",
    "        \n",
    "        # Check if final combined analysis exists - if so, we can skip to the specific model needed\n",
    "        combined_results_path = os.path.join(output_dir, f'pca_analysis_results{FILE_SUFFIX}.csv')\n",
    "        completed_models = []\n",
    "        \n",
    "        if os.path.exists(combined_results_path):\n",
    "            try:\n",
    "                combined_results = pd.read_csv(combined_results_path)\n",
    "                completed_models = combined_results['model'].unique().tolist()\n",
    "                all_model_results = [\n",
    "                    combined_results[combined_results['model'] == model] \n",
    "                    for model in completed_models\n",
    "                ]\n",
    "                print(f\"Found existing combined results with completed models: {', '.join(completed_models)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading existing combined results: {str(e)}\")\n",
    "        \n",
    "        for model_config in model_configs:\n",
    "            model_name = model_config['name']\n",
    "            \n",
    "            # Skip if this model is already fully processed\n",
    "            if model_name in completed_models and os.path.exists(model_config[\"output_file\"]):\n",
    "                print(f\"\\n\\n===== Skipping {model_name} (already fully processed) =====\")\n",
    "                \n",
    "                # Load model's component data for the comparative plot\n",
    "                try:\n",
    "                    model_summary = pd.read_csv(os.path.join(output_dir, f\"{model_name}_embeddings_summary{FILE_SUFFIX}.csv\"))\n",
    "                    model_max_components[model_name] = model_summary['n_components'].max()\n",
    "                except:\n",
    "                    # If we can't determine the max components, use a conservative default\n",
    "                    if model_name == \"glove\":\n",
    "                        model_max_components[model_name] = 300  # Using 300d as specified\n",
    "                    elif model_name == \"flair\":\n",
    "                        model_max_components[model_name] = 300  # Using 300d as specified\n",
    "                    else:\n",
    "                        model_max_components[model_name] = 400\n",
    "                        \n",
    "                continue\n",
    "                \n",
    "            print(f\"\\n\\n===== Processing with {model_name} model =====\")\n",
    "            \n",
    "            # Step 1: Check if embeddings already exist\n",
    "            if os.path.exists(model_config[\"temp_file\"]):\n",
    "                print(f\"Loading existing embeddings for {model_name} from {model_config['temp_file']}\")\n",
    "                try:\n",
    "                    all_embeddings = pd.read_pickle(model_config[\"temp_file\"])\n",
    "                    print(f\"Loaded {len(all_embeddings)} embeddings for {model_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading existing embeddings: {str(e)}\")\n",
    "                    print(\"Generating new embeddings\")\n",
    "                    all_embeddings = generate_embeddings(dataset, model_config, num_rows=3600, save_interval=100)\n",
    "            else:\n",
    "                # Generate raw embeddings (averaged text embeddings + numerical)\n",
    "                all_embeddings = generate_embeddings(dataset, model_config, num_rows=3600, save_interval=100)\n",
    "            \n",
    "            # Step 2: Analyze PCA variance patterns with checkpoint\n",
    "            variance_data, _ = analyze_pca_variance_with_checkpoint(all_embeddings, model_config)\n",
    "            \n",
    "            # Get the embedding dimension for this model\n",
    "            text_embeddings = np.vstack([emb['text_embedding'].reshape(1, -1) for emb in all_embeddings])\n",
    "            embedding_dim = text_embeddings.shape[1]\n",
    "            \n",
    "            # Determine max components for this model based on dimensionality\n",
    "            if model_name == \"glove\":\n",
    "                model_max_components[model_name] = min(embedding_dim, 300)  # Using 300d as specified\n",
    "            elif model_name == \"flair\":\n",
    "                model_max_components[model_name] = min(embedding_dim, 300)  # Using 300d as specified\n",
    "            else:\n",
    "                model_max_components[model_name] = 400\n",
    "            \n",
    "            print(f\"{model_name} has {embedding_dim} dimensions, using max {model_max_components[model_name]} components\")\n",
    "            \n",
    "            # Step 3: Generate embeddings for all PCA component options (with checkpoint)\n",
    "            component_results = generate_embeddings_for_all_components(all_embeddings, model_config)\n",
    "            all_model_results.append(component_results)\n",
    "            \n",
    "            # Step 4: Check if final embeddings already exist\n",
    "            if os.path.exists(model_config[\"output_file\"]):\n",
    "                print(f\"{model_name}: Final embeddings already exist at {model_config['output_file']}\")\n",
    "            else:\n",
    "                # Generate final embeddings with optimal components\n",
    "                optimal_components = variance_data['optimal_components']\n",
    "                print(f\"Generating final embeddings with {optimal_components} components\")\n",
    "                final_embeddings, column_names, _ = generate_pca_embeddings(all_embeddings, optimal_components)\n",
    "                \n",
    "                # Save final embeddings\n",
    "                embeddings_df = pd.DataFrame(final_embeddings, columns=column_names)\n",
    "                embeddings_df.to_csv(model_config[\"output_file\"], index=False)\n",
    "                print(f\"{model_name}: Final embeddings saved to {model_config['output_file']}\")\n",
    "        \n",
    "        # Combine all models' results for comparative analysis\n",
    "        if all_model_results:\n",
    "            combined_results = pd.concat(all_model_results)\n",
    "            \n",
    "            # Create comparative visualizations with dimensionality limits\n",
    "            comparative_plot = os.path.join(plots_dir, f'comparative_variance{FILE_SUFFIX}.png')\n",
    "            if not os.path.exists(comparative_plot) or len(all_model_results) > len(completed_models):\n",
    "                plt.figure(figsize=(12, 8))\n",
    "\n",
    "                # Fix GloVe duplicate entries\n",
    "                if 'glove' in combined_results['model'].unique():\n",
    "                    # Find duplicate components for GloVe\n",
    "                    glove_data = combined_results[combined_results['model'] == 'glove']\n",
    "                    component_counts = glove_data['n_components'].value_counts()\n",
    "                    duplicate_components = component_counts[component_counts > 1].index.tolist()\n",
    "                    \n",
    "                    # Remove first occurrence of each duplicate component\n",
    "                    for comp in duplicate_components:\n",
    "                        # Get index of first occurrence\n",
    "                        first_index = glove_data[glove_data['n_components'] == comp].index[0]\n",
    "                        # Drop it from combined_results\n",
    "                        combined_results = combined_results.drop(first_index)\n",
    "                \n",
    "                # Find the minimum x-axis limit that accommodates all models\n",
    "                min_x_limit = min(model_max_components.values()) if model_max_components else 50\n",
    "                \n",
    "                # Use matplotlib directly instead of seaborn to avoid pandas indexing issues# In the main() function, find the section creating comparative visualizations\n",
    "                comparative_plot = os.path.join(plots_dir, f'comparative_variance{FILE_SUFFIX}.png')\n",
    "                if not os.path.exists(comparative_plot) or len(all_model_results) > len(completed_models):\n",
    "                    # Change this:\n",
    "                    plt.figure(figsize=(12, 8))\n",
    "                    \n",
    "                    # To:\n",
    "                    plt.figure(figsize=(5, 3.5), dpi=300)\n",
    "                    \n",
    "                    # Find the minimum x-axis limit that accommodates all models\n",
    "                    min_x_limit = min(model_max_components.values()) if model_max_components else 50\n",
    "                    \n",
    "                    # Use matplotlib directly instead of seaborn to avoid pandas indexing issues\n",
    "                    for model in combined_results['model'].unique():\n",
    "                        # Skip gemini model if it exists\n",
    "                        if model == 'gemini':\n",
    "                            continue\n",
    "                            \n",
    "                        model_data = combined_results[combined_results['model'] == model]\n",
    "                        \n",
    "                        # Get the model's max components\n",
    "                        max_comp = model_max_components.get(model, 400)\n",
    "                        \n",
    "                        # Filter data to only include components up to the model's max\n",
    "                        model_data = model_data[model_data['n_components'] <= max_comp]\n",
    "                        \n",
    "                        x_values = model_data['n_components'].values\n",
    "                        y_values = model_data['explained_variance'].values\n",
    "                        \n",
    "                        # Sort by n_components to ensure proper line plotting\n",
    "                        indices = np.argsort(x_values)\n",
    "                        x_values = x_values[indices]\n",
    "                        y_values = y_values[indices]\n",
    "                        \n",
    "                        # Add smaller marker size and thinner lines\n",
    "                        plt.plot(x_values, y_values, marker='o', markersize=2, linewidth=1, linestyle='-', label=model)\n",
    "                    \n",
    "                    # Update title and font sizes\n",
    "                    plt.title('Explained Variance vs PCA Components', fontsize=12)\n",
    "                    plt.xlabel('Number of PCA Components', fontsize=12)\n",
    "                    plt.ylabel('Explained Variance', fontsize=12)\n",
    "                    plt.tick_params(axis='both', labelsize=8)\n",
    "                    plt.legend(fontsize=12)\n",
    "                    plt.grid(True, alpha=0.3)\n",
    "                    plt.tight_layout(pad=0.5)\n",
    "    \n",
    "                    # Save with high resolution\n",
    "                    plt.savefig(comparative_plot, dpi=300, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "                    \n",
    "                    # For the common range plot, apply the same styling changes:\n",
    "                    common_range_plot = os.path.join(plots_dir, f'comparative_variance_common_range{FILE_SUFFIX}.png')\n",
    "                    plt.figure(figsize=(5, 3.5), dpi=300)\n",
    "                    \n",
    "                    for model in combined_results['model'].unique():\n",
    "                        # Skip gemini model if it exists\n",
    "                        if model == 'gemini':\n",
    "                            continue\n",
    "                            \n",
    "                        model_data = combined_results[combined_results['model'] == model]\n",
    "                        \n",
    "                        # Filter to only include components up to the minimum x-limit\n",
    "                        model_data = model_data[model_data['n_components'] <= min_x_limit]\n",
    "                        \n",
    "                        x_values = model_data['n_components'].values\n",
    "                        y_values = model_data['explained_variance'].values\n",
    "                        \n",
    "                        # Sort by n_components to ensure proper line plotting\n",
    "                        indices = np.argsort(x_values)\n",
    "                        x_values = x_values[indices]\n",
    "                        y_values = y_values[indices]\n",
    "        \n",
    "                        # Use smaller markers and thinner lines\n",
    "                        plt.plot(x_values, y_values, marker='o', markersize=2, linewidth=1, linestyle='-', label=model)\n",
    "                    \n",
    "                    # Update title and font sizes\n",
    "                    plt.title(f'Explained Variance (0-{min_x_limit} Components)', fontsize=10)\n",
    "                    plt.xlabel('Number of PCA Components', fontsize=9)\n",
    "                    plt.ylabel('Explained Variance', fontsize=9)\n",
    "                    plt.ylim(0.6, 1)  # Set y-axis limit\n",
    "                    plt.tick_params(axis='both', labelsize=8)\n",
    "                    plt.legend(fontsize=9)\n",
    "                    plt.grid(True, alpha=0.3)\n",
    "                    plt.tight_layout(pad=0.5)\n",
    "                    \n",
    "                    # Save with high resolution\n",
    "                    plt.savefig(common_range_plot, dpi=300, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "                \n",
    "\n",
    "                \n",
    "                # Save the combined results\n",
    "                combined_results.to_csv(combined_results_path, index=False)\n",
    "        \n",
    "    except NameError:\n",
    "        print(\"Error: 'dataset' variable not found. Please load your dataset before running this script.\")\n",
    "        print(\"For example, add the following line before calling main():\")\n",
    "        print(\"dataset = pd.read_csv('your_dataset.csv')\")\n",
    "    \n",
    "    print(f\"All processing complete. Files saved with suffix: {FILE_SUFFIX}\")\n",
    "    print(f\"Output directories: {output_dir} and {plots_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1010465c-9ab2-4f06-991e-e52a619d6176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
